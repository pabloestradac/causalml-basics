{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "narrative-sailing",
      "metadata": {
        "id": "narrative-sailing"
      },
      "source": [
        "# Inference on Predictive and Causal Effects in High-Dimensional Nonlinear Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ready-appearance",
      "metadata": {
        "id": "ready-appearance"
      },
      "source": [
        "## Impact of 401(k) on  Financial Wealth\n",
        "\n",
        "As a practical illustration of the DML method, we consider estimation of the effect of 401(k) eligibility and participation\n",
        "on accumulated assets. 401(k) plans are pension accounts sponsored by employers. The key problem in determining the effect of participation in 401(k) plans on accumulated assets is saver heterogeneity coupled with the fact that the decision to enroll in a 401(k) is non-random. It is generally recognized that some people have a higher preference for saving than others. It also seems likely that those individuals with high unobserved preference for saving would be most likely to choose to participate in tax-advantaged retirement savings plans and would tend to have otherwise high amounts of accumulated assets. The presence of unobserved savings preferences with these properties then implies that conventional estimates that do not account for saver heterogeneity and endogeneity of participation will be biased upward, tending to overstate the savings effects of 401(k) participation.\n",
        "\n",
        "One can argue that eligibility for enrolling in a 401(k) plan in this data can be taken as exogenous after conditioning on a few observables of which the most important for their argument is income. The basic idea is that, at least around the time 401(k)’s initially became available, people were unlikely to be basing their employment decisions on whether an employer offered a 401(k) but would instead focus on income and other aspects of the job."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "divine-phoenix",
      "metadata": {
        "id": "divine-phoenix"
      },
      "source": [
        "### Data\n",
        "\n",
        "The data set can be downloaded from the github repo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "lj0WLB1_EwxI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj0WLB1_EwxI",
        "outputId": "741a0ac0-d9d2-4fdb-af7e-1c382874836d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=4b928888ab28c9293cf8f22b325e38920e75d6a241e6e69de8eab3637dac8935\n",
            "  Stored in directory: /Users/pablo/Library/Caches/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "75662e18",
      "metadata": {
        "id": "75662e18"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV, LinearRegression, Ridge, Lasso, LogisticRegressionCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "import patsy\n",
        "import warnings\n",
        "from sklearn.base import BaseEstimator, clone\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from IPython.display import Markdown\n",
        "import wget\n",
        "import os\n",
        "# import seaborn as sns\n",
        "warnings.simplefilter('ignore')\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6e66e77f",
      "metadata": {
        "id": "6e66e77f"
      },
      "outputs": [],
      "source": [
        "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
        "data = pd.read_csv(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "65a2d086",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "65a2d086",
        "outputId": "ed61e890-1495-43fc-8684-8c25a72fc68d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ira</th>\n",
              "      <th>a401</th>\n",
              "      <th>hval</th>\n",
              "      <th>hmort</th>\n",
              "      <th>hequity</th>\n",
              "      <th>nifa</th>\n",
              "      <th>net_nifa</th>\n",
              "      <th>tfa</th>\n",
              "      <th>net_tfa</th>\n",
              "      <th>tfa_he</th>\n",
              "      <th>...</th>\n",
              "      <th>i3</th>\n",
              "      <th>i4</th>\n",
              "      <th>i5</th>\n",
              "      <th>i6</th>\n",
              "      <th>i7</th>\n",
              "      <th>a1</th>\n",
              "      <th>a2</th>\n",
              "      <th>a3</th>\n",
              "      <th>a4</th>\n",
              "      <th>a5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9.915000e+03</td>\n",
              "      <td>9.915000e+03</td>\n",
              "      <td>9.915000e+03</td>\n",
              "      <td>9.915000e+03</td>\n",
              "      <td>9.915000e+03</td>\n",
              "      <td>...</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3462.871609</td>\n",
              "      <td>3990.459304</td>\n",
              "      <td>63595.865759</td>\n",
              "      <td>30022.505093</td>\n",
              "      <td>33573.360666</td>\n",
              "      <td>1.392864e+04</td>\n",
              "      <td>1.041415e+04</td>\n",
              "      <td>2.156603e+04</td>\n",
              "      <td>1.805153e+04</td>\n",
              "      <td>5.162490e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>0.209178</td>\n",
              "      <td>0.172668</td>\n",
              "      <td>0.121432</td>\n",
              "      <td>0.158548</td>\n",
              "      <td>0.077358</td>\n",
              "      <td>0.144024</td>\n",
              "      <td>0.209682</td>\n",
              "      <td>0.297428</td>\n",
              "      <td>0.215431</td>\n",
              "      <td>0.133434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9648.027853</td>\n",
              "      <td>12825.840676</td>\n",
              "      <td>73679.374267</td>\n",
              "      <td>40056.879558</td>\n",
              "      <td>51830.770524</td>\n",
              "      <td>5.490488e+04</td>\n",
              "      <td>5.602886e+04</td>\n",
              "      <td>6.256504e+04</td>\n",
              "      <td>6.352250e+04</td>\n",
              "      <td>9.325339e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>0.406742</td>\n",
              "      <td>0.377979</td>\n",
              "      <td>0.326645</td>\n",
              "      <td>0.365272</td>\n",
              "      <td>0.267171</td>\n",
              "      <td>0.351132</td>\n",
              "      <td>0.407102</td>\n",
              "      <td>0.457150</td>\n",
              "      <td>0.411142</td>\n",
              "      <td>0.340061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-40000.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-5.023020e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-5.023020e+05</td>\n",
              "      <td>-5.023020e+05</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000e+02</td>\n",
              "      <td>-1.350000e+03</td>\n",
              "      <td>3.750000e+02</td>\n",
              "      <td>-5.000000e+02</td>\n",
              "      <td>2.000000e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>48000.000000</td>\n",
              "      <td>7000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>1.635000e+03</td>\n",
              "      <td>2.700000e+02</td>\n",
              "      <td>3.325000e+03</td>\n",
              "      <td>1.499000e+03</td>\n",
              "      <td>1.850000e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>95000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>46650.000000</td>\n",
              "      <td>8.765500e+03</td>\n",
              "      <td>6.543500e+03</td>\n",
              "      <td>1.908450e+04</td>\n",
              "      <td>1.652450e+04</td>\n",
              "      <td>6.920000e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>100000.000000</td>\n",
              "      <td>153000.000000</td>\n",
              "      <td>300000.000000</td>\n",
              "      <td>150000.000000</td>\n",
              "      <td>300000.000000</td>\n",
              "      <td>1.430298e+06</td>\n",
              "      <td>1.430298e+06</td>\n",
              "      <td>1.536798e+06</td>\n",
              "      <td>1.536798e+06</td>\n",
              "      <td>1.687115e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ira           a401           hval          hmort  \\\n",
              "count    9915.000000    9915.000000    9915.000000    9915.000000   \n",
              "mean     3462.871609    3990.459304   63595.865759   30022.505093   \n",
              "std      9648.027853   12825.840676   73679.374267   40056.879558   \n",
              "min         0.000000       0.000000       0.000000       0.000000   \n",
              "25%         0.000000       0.000000       0.000000       0.000000   \n",
              "50%         0.000000       0.000000   48000.000000    7000.000000   \n",
              "75%         0.000000     200.000000   95000.000000   50000.000000   \n",
              "max    100000.000000  153000.000000  300000.000000  150000.000000   \n",
              "\n",
              "             hequity          nifa      net_nifa           tfa       net_tfa  \\\n",
              "count    9915.000000  9.915000e+03  9.915000e+03  9.915000e+03  9.915000e+03   \n",
              "mean    33573.360666  1.392864e+04  1.041415e+04  2.156603e+04  1.805153e+04   \n",
              "std     51830.770524  5.490488e+04  5.602886e+04  6.256504e+04  6.352250e+04   \n",
              "min    -40000.000000  0.000000e+00 -5.023020e+05  0.000000e+00 -5.023020e+05   \n",
              "25%         0.000000  2.000000e+02 -1.350000e+03  3.750000e+02 -5.000000e+02   \n",
              "50%     10000.000000  1.635000e+03  2.700000e+02  3.325000e+03  1.499000e+03   \n",
              "75%     46650.000000  8.765500e+03  6.543500e+03  1.908450e+04  1.652450e+04   \n",
              "max    300000.000000  1.430298e+06  1.430298e+06  1.536798e+06  1.536798e+06   \n",
              "\n",
              "             tfa_he  ...           i3           i4           i5           i6  \\\n",
              "count  9.915000e+03  ...  9915.000000  9915.000000  9915.000000  9915.000000   \n",
              "mean   5.162490e+04  ...     0.209178     0.172668     0.121432     0.158548   \n",
              "std    9.325339e+04  ...     0.406742     0.377979     0.326645     0.365272   \n",
              "min   -5.023020e+05  ...     0.000000     0.000000     0.000000     0.000000   \n",
              "25%    2.000000e+02  ...     0.000000     0.000000     0.000000     0.000000   \n",
              "50%    1.850000e+04  ...     0.000000     0.000000     0.000000     0.000000   \n",
              "75%    6.920000e+04  ...     0.000000     0.000000     0.000000     0.000000   \n",
              "max    1.687115e+06  ...     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "                i7           a1           a2           a3           a4  \\\n",
              "count  9915.000000  9915.000000  9915.000000  9915.000000  9915.000000   \n",
              "mean      0.077358     0.144024     0.209682     0.297428     0.215431   \n",
              "std       0.267171     0.351132     0.407102     0.457150     0.411142   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "75%       0.000000     0.000000     0.000000     1.000000     0.000000   \n",
              "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "\n",
              "                a5  \n",
              "count  9915.000000  \n",
              "mean      0.133434  \n",
              "std       0.340061  \n",
              "min       0.000000  \n",
              "25%       0.000000  \n",
              "50%       0.000000  \n",
              "75%       0.000000  \n",
              "max       1.000000  \n",
              "\n",
              "[8 rows x 44 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "00884061",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "00884061",
        "outputId": "d8896890-27fd-47c9-d0cc-69142d3bda6a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ira</th>\n",
              "      <th>a401</th>\n",
              "      <th>hval</th>\n",
              "      <th>hmort</th>\n",
              "      <th>hequity</th>\n",
              "      <th>nifa</th>\n",
              "      <th>net_nifa</th>\n",
              "      <th>tfa</th>\n",
              "      <th>net_tfa</th>\n",
              "      <th>tfa_he</th>\n",
              "      <th>...</th>\n",
              "      <th>i3</th>\n",
              "      <th>i4</th>\n",
              "      <th>i5</th>\n",
              "      <th>i6</th>\n",
              "      <th>i7</th>\n",
              "      <th>a1</th>\n",
              "      <th>a2</th>\n",
              "      <th>a3</th>\n",
              "      <th>a4</th>\n",
              "      <th>a5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>69000</td>\n",
              "      <td>60150</td>\n",
              "      <td>8850</td>\n",
              "      <td>100</td>\n",
              "      <td>-3300</td>\n",
              "      <td>100</td>\n",
              "      <td>-3300</td>\n",
              "      <td>5550</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>78000</td>\n",
              "      <td>20000</td>\n",
              "      <td>58000</td>\n",
              "      <td>61010</td>\n",
              "      <td>61010</td>\n",
              "      <td>61010</td>\n",
              "      <td>61010</td>\n",
              "      <td>119010</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1800</td>\n",
              "      <td>0</td>\n",
              "      <td>200000</td>\n",
              "      <td>15900</td>\n",
              "      <td>184100</td>\n",
              "      <td>7549</td>\n",
              "      <td>7049</td>\n",
              "      <td>9349</td>\n",
              "      <td>8849</td>\n",
              "      <td>192949</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2487</td>\n",
              "      <td>-6013</td>\n",
              "      <td>2487</td>\n",
              "      <td>-6013</td>\n",
              "      <td>-6013</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>300000</td>\n",
              "      <td>90000</td>\n",
              "      <td>210000</td>\n",
              "      <td>10625</td>\n",
              "      <td>-2375</td>\n",
              "      <td>10625</td>\n",
              "      <td>-2375</td>\n",
              "      <td>207625</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    ira  a401    hval  hmort  hequity   nifa  net_nifa    tfa  net_tfa  \\\n",
              "0     0     0   69000  60150     8850    100     -3300    100    -3300   \n",
              "1     0     0   78000  20000    58000  61010     61010  61010    61010   \n",
              "2  1800     0  200000  15900   184100   7549      7049   9349     8849   \n",
              "3     0     0       0      0        0   2487     -6013   2487    -6013   \n",
              "4     0     0  300000  90000   210000  10625     -2375  10625    -2375   \n",
              "\n",
              "   tfa_he  ...  i3  i4  i5  i6  i7  a1  a2  a3  a4  a5  \n",
              "0    5550  ...   1   0   0   0   0   0   1   0   0   0  \n",
              "1  119010  ...   0   1   0   0   0   0   0   0   1   0  \n",
              "2  192949  ...   0   0   0   1   0   0   0   0   1   0  \n",
              "3   -6013  ...   0   0   1   0   0   1   0   0   0   0  \n",
              "4  207625  ...   0   1   0   0   0   0   0   1   0   0  \n",
              "\n",
              "[5 rows x 44 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1d3dc85c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1d3dc85c",
        "outputId": "a529a234-0623-47fa-e6a4-fa5ec3207d77"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# Pension 401(k) data set\n",
              "\n",
              "## Description\n",
              "Data set on financial wealth and 401(k) plan participation. The source of this data set is the `hdm` package in `R`. It was created on 2/20/2023, by calling the `data(pension)` command in `R` after loading the `hdm` package and then writing the created dataframe to a csv file.\n",
              "\n",
              "## Format\n",
              "Dataframe with the following variables (amongst others):\n",
              "\n",
              "**p401**\n",
              "\n",
              "participation in 401(k)\n",
              "\n",
              "**e401**\n",
              "\n",
              "eligibility for 401(k)\n",
              "\n",
              "**a401**\n",
              "\n",
              "401(k) assets\n",
              "\n",
              "**tw**\n",
              "\n",
              "total wealth (in US $)\n",
              "\n",
              "**tfa**\n",
              "\n",
              "financial assets (in US $)\n",
              "\n",
              "**net_tfa**\n",
              "\n",
              "net financial assets (in US $)\n",
              "\n",
              "**nifa**\n",
              "\n",
              "non-401k financial assets (in US $)\n",
              "\n",
              "**net_nifa**\n",
              "\n",
              "net non-401k financial assets\n",
              "\n",
              "**net_n401**\n",
              "\n",
              "net non-401(k) assets (in US $)\n",
              "\n",
              "**ira**\n",
              "\n",
              "individual retirement account (IRA)\n",
              "\n",
              "**inc**\n",
              "\n",
              "income (in US $)\n",
              "\n",
              "**age**\n",
              "\n",
              "age\n",
              "\n",
              "**fsize**\n",
              "\n",
              "family size\n",
              "\n",
              "**marr**\n",
              "\n",
              "married\n",
              "\n",
              "**pira**\n",
              "\n",
              "participation in IRA\n",
              "\n",
              "**db**\n",
              "\n",
              "defined benefit pension\n",
              "\n",
              "**hown**\n",
              "\n",
              "home owner\n",
              "\n",
              "**educ**\n",
              "\n",
              "education (in years)\n",
              "\n",
              "**male**\n",
              "\n",
              "male\n",
              "\n",
              "**twoearn**\n",
              "\n",
              "two earners\n",
              "\n",
              "**nohs, hs, smcol, col**\n",
              "\n",
              "dummies for education: no high-school, high-school, some college, college\n",
              "\n",
              "**hmort**\n",
              "\n",
              "home mortage (in US $)\n",
              "\n",
              "**hequity**\n",
              "\n",
              "home equity (in US $)\n",
              "\n",
              "**hval**\n",
              "\n",
              "home value (in US $)\n",
              "\n",
              "**i1-i7**\n",
              "\n",
              "are income category indicators, \n",
              "\n",
              "**a1-a5**\n",
              "\n",
              "are age category indicators\n",
              "\n",
              "**icat** \n",
              "\n",
              "is the categorical variable that produces i1-i7. \n",
              "\n",
              "**ecat** \n",
              "\n",
              "is the categorical variable that generates education dummies (no high school, high school, some college, college). \n",
              "\n",
              "**Dum91**\n",
              "\n",
              "is a dummy for 1991 which is equal to 1 for all observations because we are only use the 1991 SIPP. \n",
              "\n",
              "**tfa_he**\n",
              "\n",
              "is net_tfa + home equity.\n",
              "\n",
              "**zhat**\n",
              "\n",
              "outcome of first stage prediction in instrumental variable analysis, from the academic paper that introduced this data\n",
              "\n",
              "\n",
              "## Details\n",
              "The sample is drawn from the 1991 Survey of Income and Program Participation (SIPP) and consists of 9,915 observations. The observational units are household reference persons aged 25-64 and spouse if present. Households are included in the sample if at least one person is employed and no one is self-employed. The data set was analysed in Chernozhukov and Hansen (2004) and Belloni et al. (2014) where further details can be found. They examine the effects of 401(k) plans on wealth using data from the Survey of Income and Program Participation using 401(k) eligibility as an instrument for 401(k) participation.\n",
              "\n",
              "## References\n",
              "V. Chernohukov, C. Hansen (2004). The impact of 401(k) participation on the wealth distribution: An instrumental quantile regression analysis. The Review of Economic and Statistics 86 (3), 735–751.\n",
              "\n",
              "A. Belloni, V. Chernozhukov, I. Fernandez-Val, and C. Hansen (2014). Program evaluation with high-dimensional data. Working Paper."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "readme = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.md\"\n",
        "filename = wget.download(readme)\n",
        "display(Markdown(open(filename, 'r').read()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "looking-invention",
      "metadata": {
        "id": "looking-invention"
      },
      "source": [
        "The data consist of 9,915 observations at the household level drawn from the 1991 Survey of Income and Program Participation (SIPP). We use net financial assets (*net\\_tfa*) as the outcome variable, $Y$,  in our analysis. The net financial assets are computed as the sum of IRA balances, 401(k) balances, checking accounts, saving bonds, other interest-earning accounts, other interest-earning assets, stocks, and mutual funds less non mortgage debts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "received-nutrition",
      "metadata": {
        "id": "received-nutrition"
      },
      "source": [
        "Among the $9915$ individuals, $3682$ are eligible to participate in the program. The variable *e401* indicates eligibility and *p401* indicates participation, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "MaLLqrNdcK09",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "MaLLqrNdcK09",
        "outputId": "30d1fa2a-0ae1-4109-ec32-3a12fdbefc41"
      },
      "outputs": [],
      "source": [
        "# sns.countplot(data, x='e401')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "material-sending",
      "metadata": {
        "id": "material-sending"
      },
      "source": [
        "Eligibility is highly associated with financial wealth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a7741bb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "a7741bb6",
        "outputId": "83253826-de29-4c45-bff6-abca0cc4da4d"
      },
      "outputs": [],
      "source": [
        "# sns.displot(data=data, x='net_tfa', kind='kde', col='e401', hue='e401', fill=True)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "awful-antigua",
      "metadata": {
        "id": "awful-antigua"
      },
      "source": [
        "The unconditional APE of e401 is about $19559$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ef070d79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef070d79",
        "outputId": "19041b19-968e-4bc5-93e4-5a03b1c3e805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19559\n"
          ]
        }
      ],
      "source": [
        "e1 = data[data['e401'] == 1]['net_tfa']\n",
        "e0 = data[data['e401'] == 0]['net_tfa']\n",
        "print(f'{np.mean(e1) - np.mean(e0):.0f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cross-priority",
      "metadata": {
        "id": "cross-priority"
      },
      "source": [
        "Among the $3682$ individuals that  are eligible, $2594$ decided to participate in the program. The unconditional APE of p401 is about $27372$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "33cd014e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33cd014e",
        "outputId": "1046c098-47f5-4edb-aa2b-027b2804652c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27372\n"
          ]
        }
      ],
      "source": [
        "e1 = data[data['p401'] == 1]['net_tfa']\n",
        "e0 = data[data['p401'] == 0]['net_tfa']\n",
        "print(f'{np.mean(e1) - np.mean(e0):.0f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "suitable-vulnerability",
      "metadata": {
        "id": "suitable-vulnerability"
      },
      "source": [
        "As discussed, these estimates are biased since they do not account for saver heterogeneity and endogeneity of participation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a9354414",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9354414",
        "outputId": "ee859f99-77e2-4bbe-dc8b-000185e64ad0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['age', 'inc', 'fsize', 'educ', 'db', 'marr', 'male', 'twoearn', 'pira',\n",
              "       'nohs', 'hs', 'smcol', 'col', 'hown'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = data['net_tfa'].values\n",
        "D = data['e401'].values\n",
        "D2 = data['p401'].values\n",
        "D3 = data['a401'].values\n",
        "X = data.drop(['e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
        "               'hval', 'hmort', 'hequity',\n",
        "               'nifa', 'net_nifa', 'net_n401', 'ira',\n",
        "               'dum91', 'icat', 'ecat', 'zhat',\n",
        "               'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7',\n",
        "               'a1', 'a2', 'a3', 'a4', 'a5'], axis=1)\n",
        "X.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c3e489",
      "metadata": {
        "id": "c4c3e489"
      },
      "source": [
        "### We define a transformer that constructs the engineered features for controls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a2cfdf4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2cfdf4c",
        "outputId": "b1475e1f-f5e4-473b-f6a5-331e4a43c464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: formulaic in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (0.6.6)\n",
            "Requirement already satisfied: astor>=0.8 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from formulaic) (0.8.1)\n",
            "Requirement already satisfied: interface-meta>=1.2.0 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from formulaic) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from formulaic) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.0 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from formulaic) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from formulaic) (1.12.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from formulaic) (4.9.0)\n",
            "Requirement already satisfied: wrapt>=1.0 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from formulaic) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from pandas>=1.0->formulaic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from pandas>=1.0->formulaic) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from pandas>=1.0->formulaic) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.0->formulaic) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install formulaic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "53b1283d",
      "metadata": {
        "id": "53b1283d"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "from formulaic import Formula\n",
        "\n",
        "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
        "\n",
        "    def __init__(self, formula, array=False):\n",
        "        self.formula = formula\n",
        "        self.array = array\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        df = Formula(self.formula).get_model_matrix(X)\n",
        "        if self.array:\n",
        "            return df.values\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "824ee320",
      "metadata": {
        "id": "824ee320"
      },
      "outputs": [],
      "source": [
        "transformer = FormulaTransformer(\"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
        "                                 \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
        "                                 \"+ male + marr + twoearn + db + pira + hown\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b35bbd34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "b35bbd34",
        "outputId": "22eabbd6-7576-4b55-fa80-2b3a510d0f4e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>poly(age, degree=6, raw=True)[0]</th>\n",
              "      <th>poly(age, degree=6, raw=True)[1]</th>\n",
              "      <th>poly(age, degree=6, raw=True)[2]</th>\n",
              "      <th>poly(age, degree=6, raw=True)[3]</th>\n",
              "      <th>poly(age, degree=6, raw=True)[4]</th>\n",
              "      <th>poly(age, degree=6, raw=True)[5]</th>\n",
              "      <th>poly(inc, degree=8, raw=True)[0]</th>\n",
              "      <th>poly(inc, degree=8, raw=True)[1]</th>\n",
              "      <th>poly(inc, degree=8, raw=True)[2]</th>\n",
              "      <th>poly(inc, degree=8, raw=True)[3]</th>\n",
              "      <th>...</th>\n",
              "      <th>poly(educ, degree=4, raw=True)[2]</th>\n",
              "      <th>poly(educ, degree=4, raw=True)[3]</th>\n",
              "      <th>poly(fsize, degree=2, raw=True)[0]</th>\n",
              "      <th>poly(fsize, degree=2, raw=True)[1]</th>\n",
              "      <th>male</th>\n",
              "      <th>marr</th>\n",
              "      <th>twoearn</th>\n",
              "      <th>db</th>\n",
              "      <th>pira</th>\n",
              "      <th>hown</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9.915000e+03</td>\n",
              "      <td>9.915000e+03</td>\n",
              "      <td>9.915000e+03</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9.915000e+03</td>\n",
              "      <td>9.915000e+03</td>\n",
              "      <td>9.915000e+03</td>\n",
              "      <td>...</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "      <td>9915.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>41.060212</td>\n",
              "      <td>1792.938981</td>\n",
              "      <td>82850.255371</td>\n",
              "      <td>4.022991e+06</td>\n",
              "      <td>2.036867e+08</td>\n",
              "      <td>1.067384e+10</td>\n",
              "      <td>37200.623197</td>\n",
              "      <td>1.997590e+09</td>\n",
              "      <td>1.437534e+14</td>\n",
              "      <td>9.104568e+17</td>\n",
              "      <td>...</td>\n",
              "      <td>2601.853757</td>\n",
              "      <td>38237.377912</td>\n",
              "      <td>2.865860</td>\n",
              "      <td>10.581241</td>\n",
              "      <td>0.205951</td>\n",
              "      <td>0.604841</td>\n",
              "      <td>0.380837</td>\n",
              "      <td>0.271004</td>\n",
              "      <td>0.242158</td>\n",
              "      <td>0.635199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>10.344505</td>\n",
              "      <td>899.134499</td>\n",
              "      <td>61725.701911</td>\n",
              "      <td>3.939869e+06</td>\n",
              "      <td>2.448502e+08</td>\n",
              "      <td>1.507229e+10</td>\n",
              "      <td>24774.288006</td>\n",
              "      <td>3.026128e+09</td>\n",
              "      <td>4.129207e+14</td>\n",
              "      <td>3.249753e+18</td>\n",
              "      <td>...</td>\n",
              "      <td>1464.892397</td>\n",
              "      <td>28259.732691</td>\n",
              "      <td>1.538937</td>\n",
              "      <td>11.391608</td>\n",
              "      <td>0.404415</td>\n",
              "      <td>0.488909</td>\n",
              "      <td>0.485617</td>\n",
              "      <td>0.444500</td>\n",
              "      <td>0.428411</td>\n",
              "      <td>0.481399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>25.000000</td>\n",
              "      <td>625.000000</td>\n",
              "      <td>15625.000000</td>\n",
              "      <td>3.906250e+05</td>\n",
              "      <td>9.765625e+06</td>\n",
              "      <td>2.441406e+08</td>\n",
              "      <td>-2652.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-1.865179e+10</td>\n",
              "      <td>-9.222694e+18</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>32.000000</td>\n",
              "      <td>1024.000000</td>\n",
              "      <td>32768.000000</td>\n",
              "      <td>1.048576e+06</td>\n",
              "      <td>3.355443e+07</td>\n",
              "      <td>1.073742e+09</td>\n",
              "      <td>19413.000000</td>\n",
              "      <td>3.768646e+08</td>\n",
              "      <td>7.316072e+12</td>\n",
              "      <td>3.677463e+16</td>\n",
              "      <td>...</td>\n",
              "      <td>1728.000000</td>\n",
              "      <td>20736.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>40.000000</td>\n",
              "      <td>1600.000000</td>\n",
              "      <td>64000.000000</td>\n",
              "      <td>2.560000e+06</td>\n",
              "      <td>1.024000e+08</td>\n",
              "      <td>4.096000e+09</td>\n",
              "      <td>31476.000000</td>\n",
              "      <td>9.907386e+08</td>\n",
              "      <td>3.118449e+13</td>\n",
              "      <td>4.226261e+17</td>\n",
              "      <td>...</td>\n",
              "      <td>1728.000000</td>\n",
              "      <td>20736.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>48.000000</td>\n",
              "      <td>2304.000000</td>\n",
              "      <td>110592.000000</td>\n",
              "      <td>5.308416e+06</td>\n",
              "      <td>2.548040e+08</td>\n",
              "      <td>1.223059e+10</td>\n",
              "      <td>48583.500000</td>\n",
              "      <td>2.360356e+09</td>\n",
              "      <td>1.146744e+14</td>\n",
              "      <td>2.117477e+18</td>\n",
              "      <td>...</td>\n",
              "      <td>4096.000000</td>\n",
              "      <td>65536.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>64.000000</td>\n",
              "      <td>4096.000000</td>\n",
              "      <td>262144.000000</td>\n",
              "      <td>1.677722e+07</td>\n",
              "      <td>1.073742e+09</td>\n",
              "      <td>6.871948e+10</td>\n",
              "      <td>242124.000000</td>\n",
              "      <td>5.862403e+10</td>\n",
              "      <td>1.419428e+16</td>\n",
              "      <td>9.222042e+18</td>\n",
              "      <td>...</td>\n",
              "      <td>5832.000000</td>\n",
              "      <td>104976.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>169.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       poly(age, degree=6, raw=True)[0]  poly(age, degree=6, raw=True)[1]  \\\n",
              "count                       9915.000000                       9915.000000   \n",
              "mean                          41.060212                       1792.938981   \n",
              "std                           10.344505                        899.134499   \n",
              "min                           25.000000                        625.000000   \n",
              "25%                           32.000000                       1024.000000   \n",
              "50%                           40.000000                       1600.000000   \n",
              "75%                           48.000000                       2304.000000   \n",
              "max                           64.000000                       4096.000000   \n",
              "\n",
              "       poly(age, degree=6, raw=True)[2]  poly(age, degree=6, raw=True)[3]  \\\n",
              "count                       9915.000000                      9.915000e+03   \n",
              "mean                       82850.255371                      4.022991e+06   \n",
              "std                        61725.701911                      3.939869e+06   \n",
              "min                        15625.000000                      3.906250e+05   \n",
              "25%                        32768.000000                      1.048576e+06   \n",
              "50%                        64000.000000                      2.560000e+06   \n",
              "75%                       110592.000000                      5.308416e+06   \n",
              "max                       262144.000000                      1.677722e+07   \n",
              "\n",
              "       poly(age, degree=6, raw=True)[4]  poly(age, degree=6, raw=True)[5]  \\\n",
              "count                      9.915000e+03                      9.915000e+03   \n",
              "mean                       2.036867e+08                      1.067384e+10   \n",
              "std                        2.448502e+08                      1.507229e+10   \n",
              "min                        9.765625e+06                      2.441406e+08   \n",
              "25%                        3.355443e+07                      1.073742e+09   \n",
              "50%                        1.024000e+08                      4.096000e+09   \n",
              "75%                        2.548040e+08                      1.223059e+10   \n",
              "max                        1.073742e+09                      6.871948e+10   \n",
              "\n",
              "       poly(inc, degree=8, raw=True)[0]  poly(inc, degree=8, raw=True)[1]  \\\n",
              "count                       9915.000000                      9.915000e+03   \n",
              "mean                       37200.623197                      1.997590e+09   \n",
              "std                        24774.288006                      3.026128e+09   \n",
              "min                        -2652.000000                      0.000000e+00   \n",
              "25%                        19413.000000                      3.768646e+08   \n",
              "50%                        31476.000000                      9.907386e+08   \n",
              "75%                        48583.500000                      2.360356e+09   \n",
              "max                       242124.000000                      5.862403e+10   \n",
              "\n",
              "       poly(inc, degree=8, raw=True)[2]  poly(inc, degree=8, raw=True)[3]  \\\n",
              "count                      9.915000e+03                      9.915000e+03   \n",
              "mean                       1.437534e+14                      9.104568e+17   \n",
              "std                        4.129207e+14                      3.249753e+18   \n",
              "min                       -1.865179e+10                     -9.222694e+18   \n",
              "25%                        7.316072e+12                      3.677463e+16   \n",
              "50%                        3.118449e+13                      4.226261e+17   \n",
              "75%                        1.146744e+14                      2.117477e+18   \n",
              "max                        1.419428e+16                      9.222042e+18   \n",
              "\n",
              "       ...  poly(educ, degree=4, raw=True)[2]  \\\n",
              "count  ...                        9915.000000   \n",
              "mean   ...                        2601.853757   \n",
              "std    ...                        1464.892397   \n",
              "min    ...                           1.000000   \n",
              "25%    ...                        1728.000000   \n",
              "50%    ...                        1728.000000   \n",
              "75%    ...                        4096.000000   \n",
              "max    ...                        5832.000000   \n",
              "\n",
              "       poly(educ, degree=4, raw=True)[3]  poly(fsize, degree=2, raw=True)[0]  \\\n",
              "count                        9915.000000                         9915.000000   \n",
              "mean                        38237.377912                            2.865860   \n",
              "std                         28259.732691                            1.538937   \n",
              "min                             1.000000                            1.000000   \n",
              "25%                         20736.000000                            2.000000   \n",
              "50%                         20736.000000                            3.000000   \n",
              "75%                         65536.000000                            4.000000   \n",
              "max                        104976.000000                           13.000000   \n",
              "\n",
              "       poly(fsize, degree=2, raw=True)[1]         male         marr  \\\n",
              "count                         9915.000000  9915.000000  9915.000000   \n",
              "mean                            10.581241     0.205951     0.604841   \n",
              "std                             11.391608     0.404415     0.488909   \n",
              "min                              1.000000     0.000000     0.000000   \n",
              "25%                              4.000000     0.000000     0.000000   \n",
              "50%                              9.000000     0.000000     1.000000   \n",
              "75%                             16.000000     0.000000     1.000000   \n",
              "max                            169.000000     1.000000     1.000000   \n",
              "\n",
              "           twoearn           db         pira         hown  \n",
              "count  9915.000000  9915.000000  9915.000000  9915.000000  \n",
              "mean      0.380837     0.271004     0.242158     0.635199  \n",
              "std       0.485617     0.444500     0.428411     0.481399  \n",
              "min       0.000000     0.000000     0.000000     0.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000  \n",
              "50%       0.000000     0.000000     0.000000     1.000000  \n",
              "75%       1.000000     1.000000     0.000000     1.000000  \n",
              "max       1.000000     1.000000     1.000000     1.000000  \n",
              "\n",
              "[8 rows x 26 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer.fit_transform(X).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8c5e20e7",
      "metadata": {
        "id": "8c5e20e7"
      },
      "outputs": [],
      "source": [
        "transformer = FormulaTransformer(\"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
        "                                 \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
        "                                 \"+ male + marr + twoearn + db + pira + hown\", array=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a745a6f2",
      "metadata": {
        "id": "a745a6f2"
      },
      "source": [
        "## Estimating the ATE of 401(k) Eligibility on Net Financial Assets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10538ad3",
      "metadata": {
        "id": "10538ad3"
      },
      "source": [
        "We are interested in valid estimators of the average treatment effect of `e401` and `p401` on `net_tfa`. We start using ML approaches to estimate the function $g_0$ and $m_0$ in the following PLR model:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fcf3165",
      "metadata": {
        "id": "6fcf3165"
      },
      "source": [
        "$$\n",
        "\\begin{align}\n",
        "Y &= D\\theta_0 + g_0(X) + \\zeta, & E[\\zeta \\mid D,X]&= 0,\\\\\n",
        "D &= m_0(X) + V, & E[V \\mid X] &= 0.\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d198da92",
      "metadata": {
        "id": "d198da92"
      },
      "source": [
        "### Double ML in PLR with Cross-Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "045487a5",
      "metadata": {
        "id": "045487a5"
      },
      "source": [
        "We define a simple dml function that is parameterized by arbitrary ML models and returns the treatment effect and other useful quantities of the analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1ee13ea8",
      "metadata": {
        "id": "1ee13ea8"
      },
      "outputs": [],
      "source": [
        "def dml(X, D, y, modely, modeld, *, nfolds, classifier=False):\n",
        "    '''\n",
        "    DML for the Partially Linear Model setting with cross-fitting\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "    X: the controls\n",
        "    D: the treatment\n",
        "    y: the outcome\n",
        "    modely: the ML model for predicting the outcome y\n",
        "    modeld: the ML model for predicting the treatment D\n",
        "    nfolds: the number of folds in cross-fitting\n",
        "    classifier: bool, whether the modeld is a classifier or a regressor\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "    point: the point estimate of the treatment effect of D on y\n",
        "    stderr: the standard error of the treatment effect\n",
        "    yhat: the cross-fitted predictions for the outcome y\n",
        "    Dhat: the cross-fitted predictions for the treatment D\n",
        "    resy: the outcome residuals\n",
        "    resD: the treatment residuals\n",
        "    epsilon: the final residual-on-residual OLS regression residual\n",
        "    '''\n",
        "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123) # shuffled k-folds\n",
        "    yhat = cross_val_predict(modely, X, y, cv=cv, n_jobs=-1) # out-of-fold predictions for y\n",
        "    # out-of-fold predictions for D\n",
        "    # use predict or predict_proba dependent on classifier or regressor for D\n",
        "    if classifier:\n",
        "        Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
        "    else:\n",
        "        Dhat = cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1)\n",
        "    # calculate outcome and treatment residuals\n",
        "    resy = y - yhat\n",
        "    resD = D - Dhat\n",
        "\n",
        "    # final stage ols based point estimate and standard error\n",
        "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
        "    epsilon = resy - point * resD\n",
        "    var = np.mean(epsilon**2 * resD**2) / np.mean(resD**2)**2\n",
        "    stderr = np.sqrt(var / X.shape[0])\n",
        "\n",
        "    return point, stderr, yhat, Dhat, resy, resD, epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "35c70f74",
      "metadata": {
        "id": "35c70f74"
      },
      "outputs": [],
      "source": [
        "def summary(point, stderr, yhat, Dhat, resy, resD, epsilon, X, D, y, *, name):\n",
        "    '''\n",
        "    Convenience summary function that takes the results of the DML function\n",
        "    and summarizes several estimation quantities and performance metrics.\n",
        "    '''\n",
        "    return pd.DataFrame({'estimate': point, # point estimate\n",
        "                         'stderr': stderr, # standard error\n",
        "                         'lower': point - 1.96*stderr, # lower end of 95% confidence interval\n",
        "                         'upper': point + 1.96*stderr, # upper end of 95% confidence interval\n",
        "                         'rmse y': np.sqrt(np.mean(resy**2)), # RMSE of model that predicts outcome y\n",
        "                         'rmse D': np.sqrt(np.mean(resD**2)), # RMSE of model that predicts treatment D\n",
        "                         'accuracy D': np.mean(np.abs(resD) < .5), # binary classification accuracy of model for D\n",
        "                         }, index=[name])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e6276a",
      "metadata": {
        "id": "b0e6276a"
      },
      "source": [
        "#### Double Lasso with Cross-Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "21f57b4e",
      "metadata": {
        "id": "21f57b4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4842524142.347656, tolerance: 2793965476.3442807\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5729074177.640625, tolerance: 2793965476.3442807\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3703135593.5898438, tolerance: 2029881536.0246825\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6263968227.46875, tolerance: 2029881536.0246825\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8077687350.232422, tolerance: 2029881536.0246825\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5338872752.689453, tolerance: 2528202817.312562\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20003626681000242, tolerance: 0.1475441134751773\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.24790538668298723, tolerance: 0.1475441134751773\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.27270004554384286, tolerance: 0.1475441134751773\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2843818607200319, tolerance: 0.1475441134751773\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2272922653223759, tolerance: 0.1475441134751773\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4713927065615735, tolerance: 0.1475441134751773\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6586446264507231, tolerance: 0.1475441134751773\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7847971172377584, tolerance: 0.1475441134751773\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8648985255977095, tolerance: 0.1475441134751773\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.913992759060875, tolerance: 0.1475441134751773\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9432523170321474, tolerance: 0.1475441134751773\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15897930532992177, tolerance: 0.1483228368794327\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2193124103282571, tolerance: 0.1483228368794327\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2522481327018795, tolerance: 0.1483228368794327\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.26890943649300425, tolerance: 0.1483228368794327\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.27685410284880163, tolerance: 0.1483228368794327\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28070747937022134, tolerance: 0.1483228368794327\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2841087811386842, tolerance: 0.1483228368794327\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22022668664612866, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2605810182622008, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2812674891356437, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.18918432886835035, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.46258894503534975, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6832765152128104, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8355900195997492, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9339333853959033, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20731427069927122, tolerance: 0.14823447841159795\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23987029684690242, tolerance: 0.14823447841159795\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.25629786640911334, tolerance: 0.14823447841159795\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2639774822628169, tolerance: 0.14823447841159795\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2672962439892217, tolerance: 0.14823447841159795\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2750191345553503, tolerance: 0.14823447841159795\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2842220805614488, tolerance: 0.14823447841159795\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.150140356578504, tolerance: 0.14869057674125435\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.20644618033543338, tolerance: 0.14869057674125435\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23788562213258047, tolerance: 0.14869057674125435\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.25412077259579746, tolerance: 0.14869057674125435\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2619976204434806, tolerance: 0.14869057674125435\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.26548183348745624, tolerance: 0.14869057674125435\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.577e+00, tolerance: 1.853e-01\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.21885669810262698, tolerance: 0.1473049645390066\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.25962592520181715, tolerance: 0.1473049645390066\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.15105345298570683, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2802727696525835, tolerance: 0.1473049645390066\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22563746663081474, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.26657582787743195, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.18565424825715127, tolerance: 0.14719785657998474\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28674385655517654, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2400227643818198, tolerance: 0.14719785657998474\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2892355395292725, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.30074514550528875, tolerance: 0.1473049645390066\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.26878965804644395, tolerance: 0.14719785657998474\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5248564577048, tolerance: 0.1473049645390066\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28296262684489193, tolerance: 0.14719785657998474\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3707772252766972, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.686920880792286, tolerance: 0.1473049645390066\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2893689165500746, tolerance: 0.14719785657998474\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5743004053119876, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7932214191735056, tolerance: 0.1473049645390066\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7135749120413948, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8594420863003052, tolerance: 0.1473049645390066\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.801734148908281, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8551481926106135, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8864588014675974, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2888197964603023, tolerance: 0.14719785657998474\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9052700713918966, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9438612906258186, tolerance: 0.14822064617809289\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2318268074864136, tolerance: 0.1481693617021278\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.27526050284313897, tolerance: 0.1481693617021278\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4110660241947244, tolerance: 0.1481693617021278\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6249263608656292, tolerance: 0.1481693617021278\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.771704287340981, tolerance: 0.1481693617021278\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2024485808465215, tolerance: 0.14892539007092195\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.24616535471477619, tolerance: 0.14892539007092195\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2685910074719686, tolerance: 0.14892539007092195\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.27932055565270275, tolerance: 0.14892539007092195\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.28395750344998305, tolerance: 0.14892539007092195\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8995013584935805, tolerance: 0.1481693617021278\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8427850376456263, tolerance: 0.1481693617021278\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.750273868857221, tolerance: 0.1481693617021278\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2778393828100434, tolerance: 0.14892539007092195\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.537626253102644, tolerance: 0.1481693617021278\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5093018786078574, tolerance: 0.14892539007092195\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.17696587646401, tolerance: 0.1481693617021278\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2147336170901326, tolerance: 0.14784659628112276\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2253601841434829, tolerance: 0.14800260006303226\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.25982102094667425, tolerance: 0.14784659628112276\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1834797089377389, tolerance: 0.14891479672234442\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.26594787932845065, tolerance: 0.14800260006303226\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2834927900553339, tolerance: 0.14784659628112276\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2303950629202518, tolerance: 0.14891479672234442\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.29526353497362834, tolerance: 0.14784659628112276\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2546982754770397, tolerance: 0.14891479672234442\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3007339762521042, tolerance: 0.14784659628112276\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.31592863718628905, tolerance: 0.14800260006303226\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2544513279963212, tolerance: 0.14891479672234442\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5465602867159305, tolerance: 0.14800260006303226\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2523134539083003, tolerance: 0.14891479672234442\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7128786744628997, tolerance: 0.14800260006303226\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8235967889831954, tolerance: 0.14800260006303226\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22857334414288744, tolerance: 0.14891479672234442\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9063431487434173, tolerance: 0.14800260006303226\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.44586481922783605, tolerance: 0.14891479672234442\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6080170007585366, tolerance: 0.14891479672234442\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7352824751471871, tolerance: 0.14891479672234442\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.81868075352304, tolerance: 0.14891479672234442\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8699945987486899, tolerance: 0.14891479672234442\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.44523652395969293, tolerance: 0.14800260006303226\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19753904518620402, tolerance: 0.1475046958714141\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.25733426336159937, tolerance: 0.1475046958714141\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2894048437581205, tolerance: 0.1475046958714141\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.30499664931517145, tolerance: 0.1475046958714141\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1705827903529098, tolerance: 0.14841306334699095\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.21107820833526603, tolerance: 0.14841306334699095\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.31360589780524606, tolerance: 0.1475046958714141\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23836921089468888, tolerance: 0.14841306334699095\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.55236349719803, tolerance: 0.1475046958714141\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.27568042805069126, tolerance: 0.14841306334699095\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7463393846423969, tolerance: 0.1475046958714141\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22034755585286803, tolerance: 0.14841306334699095\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8792118152614421, tolerance: 0.1475046958714141\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9641946796211869, tolerance: 0.1475046958714141\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0166308872076115, tolerance: 0.1475046958714141\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0480577744012862, tolerance: 0.1475046958714141\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.24151775617997373, tolerance: 0.14823447841159795\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1987456618721808, tolerance: 0.14823447841159795\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.163817450307306, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22087842614246256, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2554117218480769, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2745456059121807, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2594829728047898, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4819061638588664, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6447228072283906, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7517226623569968, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.818350220804632, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8583968861080393, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8817159963552967, tolerance: 0.14838764576111013\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.595e+00, tolerance: 1.847e-01\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.106e+00, tolerance: 1.859e-01\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.570e-01, tolerance: 1.848e-01\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ],
      "source": [
        "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
        "lassod = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
        "result = dml(X, D, y, lassoy, lassod, nfolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "265fb305",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "265fb305",
        "outputId": "6c3fd7bf-10d0-4d29-87b1-6ab3f0339702"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimate</th>\n",
              "      <th>stderr</th>\n",
              "      <th>lower</th>\n",
              "      <th>upper</th>\n",
              "      <th>rmse y</th>\n",
              "      <th>rmse D</th>\n",
              "      <th>accuracy D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>double lasso</th>\n",
              "      <td>9035.120004</td>\n",
              "      <td>1295.135748</td>\n",
              "      <td>6496.653938</td>\n",
              "      <td>11573.58607</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.443406</td>\n",
              "      <td>0.688553</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 estimate       stderr        lower        upper  \\\n",
              "double lasso  9035.120004  1295.135748  6496.653938  11573.58607   \n",
              "\n",
              "                    rmse y    rmse D  accuracy D  \n",
              "double lasso  54254.468883  0.443406    0.688553  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table = summary(*result, X, D, y, name='double lasso')\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30cb77e8",
      "metadata": {
        "id": "30cb77e8"
      },
      "source": [
        "#### Using a Penalized Logistic Regression for D\n",
        "\n",
        "Note the default logistic regression uses an $\\ell_2$ penalty. You can use the $\\ell_1$ penalty as well, but computation will take longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ba3ba66e",
      "metadata": {
        "id": "ba3ba66e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4842524142.347656, tolerance: 2793965476.3442807\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5729074177.640625, tolerance: 2793965476.3442807\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3703135593.5898438, tolerance: 2029881536.0246825\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6263968227.46875, tolerance: 2029881536.0246825\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8077687350.232422, tolerance: 2029881536.0246825\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:664: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5338872752.689453, tolerance: 2528202817.312562\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n"
          ]
        }
      ],
      "source": [
        "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
        "lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
        "result = dml(X, D, y, lassoy, lgrd, nfolds=5, classifier=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "WFODGyZX4io9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "WFODGyZX4io9",
        "outputId": "bcb26861-a736-4948-b61a-39b71477cff7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimate</th>\n",
              "      <th>stderr</th>\n",
              "      <th>lower</th>\n",
              "      <th>upper</th>\n",
              "      <th>rmse y</th>\n",
              "      <th>rmse D</th>\n",
              "      <th>accuracy D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lasso/logistic</th>\n",
              "      <td>9092.461173</td>\n",
              "      <td>1304.397656</td>\n",
              "      <td>6535.841766</td>\n",
              "      <td>11649.080579</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.444043</td>\n",
              "      <td>0.687847</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   estimate       stderr        lower         upper  \\\n",
              "lasso/logistic  9092.461173  1304.397656  6535.841766  11649.080579   \n",
              "\n",
              "                      rmse y    rmse D  accuracy D  \n",
              "lasso/logistic  54254.468883  0.444043    0.687847  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(*result,  X, D, y, name='lasso/logistic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "cbbaa344",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "cbbaa344",
        "outputId": "4c46aef0-7821-4588-988a-5eeb8d9373c0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimate</th>\n",
              "      <th>stderr</th>\n",
              "      <th>lower</th>\n",
              "      <th>upper</th>\n",
              "      <th>rmse y</th>\n",
              "      <th>rmse D</th>\n",
              "      <th>accuracy D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>double lasso</th>\n",
              "      <td>9035.120004</td>\n",
              "      <td>1295.135748</td>\n",
              "      <td>6496.653938</td>\n",
              "      <td>11573.586070</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.443406</td>\n",
              "      <td>0.688553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso/logistic</th>\n",
              "      <td>9092.461173</td>\n",
              "      <td>1304.397656</td>\n",
              "      <td>6535.841766</td>\n",
              "      <td>11649.080579</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.444043</td>\n",
              "      <td>0.687847</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   estimate       stderr        lower         upper  \\\n",
              "double lasso    9035.120004  1295.135748  6496.653938  11573.586070   \n",
              "lasso/logistic  9092.461173  1304.397656  6535.841766  11649.080579   \n",
              "\n",
              "                      rmse y    rmse D  accuracy D  \n",
              "double lasso    54254.468883  0.443406    0.688553  \n",
              "lasso/logistic  54254.468883  0.444043    0.687847  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table = pd.concat([table , summary(*result,  X, D, y, name='lasso/logistic')])\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fluid-gregory",
      "metadata": {
        "id": "fluid-gregory"
      },
      "source": [
        "Then, we repeat this procedure for various machine learning methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4791084",
      "metadata": {
        "id": "c4791084"
      },
      "source": [
        "### Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "de0d4030",
      "metadata": {
        "id": "de0d4030"
      },
      "outputs": [],
      "source": [
        "rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
        "rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
        "result = dml(X, D, y, rfy, rfd, nfolds=5, classifier=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e5073e3a",
      "metadata": {
        "id": "e5073e3a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimate</th>\n",
              "      <th>stderr</th>\n",
              "      <th>lower</th>\n",
              "      <th>upper</th>\n",
              "      <th>rmse y</th>\n",
              "      <th>rmse D</th>\n",
              "      <th>accuracy D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>double lasso</th>\n",
              "      <td>9035.120004</td>\n",
              "      <td>1295.135748</td>\n",
              "      <td>6496.653938</td>\n",
              "      <td>11573.586070</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.443406</td>\n",
              "      <td>0.688553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso/logistic</th>\n",
              "      <td>9092.461173</td>\n",
              "      <td>1304.397656</td>\n",
              "      <td>6535.841766</td>\n",
              "      <td>11649.080579</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.444043</td>\n",
              "      <td>0.687847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>random forest</th>\n",
              "      <td>8823.389678</td>\n",
              "      <td>1360.309746</td>\n",
              "      <td>6157.182577</td>\n",
              "      <td>11489.596779</td>\n",
              "      <td>55152.962053</td>\n",
              "      <td>0.444324</td>\n",
              "      <td>0.688553</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   estimate       stderr        lower         upper  \\\n",
              "double lasso    9035.120004  1295.135748  6496.653938  11573.586070   \n",
              "lasso/logistic  9092.461173  1304.397656  6535.841766  11649.080579   \n",
              "random forest   8823.389678  1360.309746  6157.182577  11489.596779   \n",
              "\n",
              "                      rmse y    rmse D  accuracy D  \n",
              "double lasso    54254.468883  0.443406    0.688553  \n",
              "lasso/logistic  54254.468883  0.444043    0.687847  \n",
              "random forest   55152.962053  0.444324    0.688553  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table = pd.concat([table , summary(*result,  X, D, y, name='random forest')])\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1adab609",
      "metadata": {
        "id": "1adab609"
      },
      "source": [
        "### Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e3a3266c",
      "metadata": {
        "id": "e3a3266c"
      },
      "outputs": [],
      "source": [
        "dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
        "dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
        "result = dml(X, D, y, dtry, dtrd, nfolds=5, classifier=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e82ddc9f",
      "metadata": {
        "id": "e82ddc9f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimate</th>\n",
              "      <th>stderr</th>\n",
              "      <th>lower</th>\n",
              "      <th>upper</th>\n",
              "      <th>rmse y</th>\n",
              "      <th>rmse D</th>\n",
              "      <th>accuracy D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>double lasso</th>\n",
              "      <td>9035.120004</td>\n",
              "      <td>1295.135748</td>\n",
              "      <td>6496.653938</td>\n",
              "      <td>11573.586070</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.443406</td>\n",
              "      <td>0.688553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso/logistic</th>\n",
              "      <td>9092.461173</td>\n",
              "      <td>1304.397656</td>\n",
              "      <td>6535.841766</td>\n",
              "      <td>11649.080579</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.444043</td>\n",
              "      <td>0.687847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>random forest</th>\n",
              "      <td>8823.389678</td>\n",
              "      <td>1360.309746</td>\n",
              "      <td>6157.182577</td>\n",
              "      <td>11489.596779</td>\n",
              "      <td>55152.962053</td>\n",
              "      <td>0.444324</td>\n",
              "      <td>0.688553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decision tree</th>\n",
              "      <td>9237.030821</td>\n",
              "      <td>1440.550571</td>\n",
              "      <td>6413.551700</td>\n",
              "      <td>12060.509941</td>\n",
              "      <td>59427.226868</td>\n",
              "      <td>0.446437</td>\n",
              "      <td>0.688048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   estimate       stderr        lower         upper  \\\n",
              "double lasso    9035.120004  1295.135748  6496.653938  11573.586070   \n",
              "lasso/logistic  9092.461173  1304.397656  6535.841766  11649.080579   \n",
              "random forest   8823.389678  1360.309746  6157.182577  11489.596779   \n",
              "decision tree   9237.030821  1440.550571  6413.551700  12060.509941   \n",
              "\n",
              "                      rmse y    rmse D  accuracy D  \n",
              "double lasso    54254.468883  0.443406    0.688553  \n",
              "lasso/logistic  54254.468883  0.444043    0.687847  \n",
              "random forest   55152.962053  0.444324    0.688553  \n",
              "decision tree   59427.226868  0.446437    0.688048  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table = pd.concat([table , summary(*result,  X, D, y, name='decision tree')])\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "415d5b98",
      "metadata": {
        "id": "415d5b98"
      },
      "source": [
        "### Boosted Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9016b1a8",
      "metadata": {
        "id": "9016b1a8"
      },
      "outputs": [],
      "source": [
        "gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
        "gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
        "result = dml(X, D, y, gbfy, gbfd, nfolds=5, classifier=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "10f4e992",
      "metadata": {
        "id": "10f4e992"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimate</th>\n",
              "      <th>stderr</th>\n",
              "      <th>lower</th>\n",
              "      <th>upper</th>\n",
              "      <th>rmse y</th>\n",
              "      <th>rmse D</th>\n",
              "      <th>accuracy D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>double lasso</th>\n",
              "      <td>9035.120004</td>\n",
              "      <td>1295.135748</td>\n",
              "      <td>6496.653938</td>\n",
              "      <td>11573.586070</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.443406</td>\n",
              "      <td>0.688553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso/logistic</th>\n",
              "      <td>9092.461173</td>\n",
              "      <td>1304.397656</td>\n",
              "      <td>6535.841766</td>\n",
              "      <td>11649.080579</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.444043</td>\n",
              "      <td>0.687847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>random forest</th>\n",
              "      <td>8823.389678</td>\n",
              "      <td>1360.309746</td>\n",
              "      <td>6157.182577</td>\n",
              "      <td>11489.596779</td>\n",
              "      <td>55152.962053</td>\n",
              "      <td>0.444324</td>\n",
              "      <td>0.688553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decision tree</th>\n",
              "      <td>9237.030821</td>\n",
              "      <td>1440.550571</td>\n",
              "      <td>6413.551700</td>\n",
              "      <td>12060.509941</td>\n",
              "      <td>59427.226868</td>\n",
              "      <td>0.446437</td>\n",
              "      <td>0.688048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>boosted forest</th>\n",
              "      <td>9082.510152</td>\n",
              "      <td>1336.345327</td>\n",
              "      <td>6463.273310</td>\n",
              "      <td>11701.746994</td>\n",
              "      <td>55593.735380</td>\n",
              "      <td>0.443375</td>\n",
              "      <td>0.690066</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   estimate       stderr        lower         upper  \\\n",
              "double lasso    9035.120004  1295.135748  6496.653938  11573.586070   \n",
              "lasso/logistic  9092.461173  1304.397656  6535.841766  11649.080579   \n",
              "random forest   8823.389678  1360.309746  6157.182577  11489.596779   \n",
              "decision tree   9237.030821  1440.550571  6413.551700  12060.509941   \n",
              "boosted forest  9082.510152  1336.345327  6463.273310  11701.746994   \n",
              "\n",
              "                      rmse y    rmse D  accuracy D  \n",
              "double lasso    54254.468883  0.443406    0.688553  \n",
              "lasso/logistic  54254.468883  0.444043    0.687847  \n",
              "random forest   55152.962053  0.444324    0.688553  \n",
              "decision tree   59427.226868  0.446437    0.688048  \n",
              "boosted forest  55593.735380  0.443375    0.690066  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table = pd.concat([table , summary(*result,  X, D, y, name='boosted forest')])\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "identical-smith",
      "metadata": {
        "id": "identical-smith"
      },
      "source": [
        "The best model with lowest RMSE in both equation is the PLR model estimated via lasso."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c8dc0c4",
      "metadata": {
        "id": "9c8dc0c4"
      },
      "source": [
        "### AutoML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "r_kFRdEyFjQc",
      "metadata": {
        "id": "r_kFRdEyFjQc"
      },
      "outputs": [],
      "source": [
        "# !pip install flaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "2d0d2b4e",
      "metadata": {
        "id": "2d0d2b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[flaml.automl.logger: 06-30 11:15:30] {1680} INFO - task = regression\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1680} INFO - task = regression\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1680} INFO - task = regression\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1789} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1789} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1789} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1680} INFO - task = regression\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1680} INFO - task = regression\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1789} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1789} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2345} INFO - Estimated sufficient time budget=1579s. Estimated necessary time budget=11s.\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.2s,\testimator lgbm's best error=0.8864,\tbest estimator lgbm's best error=0.8864\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2345} INFO - Estimated sufficient time budget=1570s. Estimated necessary time budget=11s.\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.2s,\testimator lgbm's best error=0.9033,\tbest estimator lgbm's best error=0.9033\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2345} INFO - Estimated sufficient time budget=1580s. Estimated necessary time budget=11s.\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.2s,\testimator lgbm's best error=0.8910,\tbest estimator lgbm's best error=0.8910\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2345} INFO - Estimated sufficient time budget=1621s. Estimated necessary time budget=11s.\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.2s,\testimator lgbm's best error=0.8744,\tbest estimator lgbm's best error=0.8744\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2345} INFO - Estimated sufficient time budget=1843s. Estimated necessary time budget=13s.\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.2s,\testimator lgbm's best error=0.8782,\tbest estimator lgbm's best error=0.8782\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.3s,\testimator lgbm's best error=0.8910,\tbest estimator lgbm's best error=0.8910\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.3s,\testimator lgbm's best error=0.9033,\tbest estimator lgbm's best error=0.9033\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.3s,\testimator lgbm's best error=0.8864,\tbest estimator lgbm's best error=0.8864\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.3s,\testimator lgbm's best error=0.8744,\tbest estimator lgbm's best error=0.8744\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.3s,\testimator lgbm's best error=0.8782,\tbest estimator lgbm's best error=0.8782\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.4s,\testimator lgbm's best error=0.8184,\tbest estimator lgbm's best error=0.8184\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.4s,\testimator lgbm's best error=0.8483,\tbest estimator lgbm's best error=0.8483\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.4s,\testimator lgbm's best error=0.8104,\tbest estimator lgbm's best error=0.8104\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.4s,\testimator lgbm's best error=0.8011,\tbest estimator lgbm's best error=0.8011\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 3, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.4s,\testimator lgbm's best error=0.7994,\tbest estimator lgbm's best error=0.7994\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 3, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.5s,\testimator xgboost's best error=0.8773,\tbest estimator lgbm's best error=0.8184\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.5s,\testimator xgboost's best error=0.9186,\tbest estimator lgbm's best error=0.8483\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.5s,\testimator xgboost's best error=0.8912,\tbest estimator lgbm's best error=0.8104\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2392} INFO -  at 0.6s,\testimator lgbm's best error=0.7494,\tbest estimator lgbm's best error=0.7494\n",
            "[flaml.automl.logger: 06-30 11:15:30] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 0.7s,\testimator lgbm's best error=0.7724,\tbest estimator lgbm's best error=0.7724\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 0.7s,\testimator lgbm's best error=0.7494,\tbest estimator lgbm's best error=0.7494\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 0.7s,\testimator lgbm's best error=0.7919,\tbest estimator lgbm's best error=0.7919\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 0.7s,\testimator lgbm's best error=0.8184,\tbest estimator lgbm's best error=0.8184\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 5, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 0.7s,\testimator lgbm's best error=0.8285,\tbest estimator lgbm's best error=0.8285\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 0.8s,\testimator lgbm's best error=0.7724,\tbest estimator lgbm's best error=0.7724\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 0.8s,\testimator lgbm's best error=0.7919,\tbest estimator lgbm's best error=0.7919\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 0.8s,\testimator lgbm's best error=0.8285,\tbest estimator lgbm's best error=0.8285\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 0.9s,\testimator xgboost's best error=0.8773,\tbest estimator lgbm's best error=0.8184\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 0.8s,\testimator lgbm's best error=0.7494,\tbest estimator lgbm's best error=0.7494\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 6, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.0s,\testimator lgbm's best error=0.7724,\tbest estimator lgbm's best error=0.7724\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 6, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.0s,\testimator lgbm's best error=0.8184,\tbest estimator lgbm's best error=0.8184\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.0s,\testimator xgboost's best error=0.8900,\tbest estimator lgbm's best error=0.7494\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.1s,\testimator lgbm's best error=0.8285,\tbest estimator lgbm's best error=0.8285\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.1s,\testimator lgbm's best error=0.7919,\tbest estimator lgbm's best error=0.7919\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.1s,\testimator lgbm's best error=0.7712,\tbest estimator lgbm's best error=0.7712\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.1s,\testimator xgboost's best error=0.8739,\tbest estimator lgbm's best error=0.7724\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.2s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 8, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.2s,\testimator lgbm's best error=0.8095,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 8, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.2s,\testimator lgbm's best error=0.7163,\tbest estimator lgbm's best error=0.7163\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.2s,\testimator lgbm's best error=0.7712,\tbest estimator lgbm's best error=0.7712\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.3s,\testimator lgbm's best error=0.7497,\tbest estimator lgbm's best error=0.7497\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.3s,\testimator xgboost's best error=0.9186,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.3s,\testimator xgboost's best error=0.8912,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.3s,\testimator lgbm's best error=0.7163,\tbest estimator lgbm's best error=0.7163\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.3s,\testimator xgboost's best error=0.8159,\tbest estimator lgbm's best error=0.7712\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 10, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.3s,\testimator lgbm's best error=0.7497,\tbest estimator lgbm's best error=0.7497\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.4s,\testimator xgboost's best error=0.8645,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 10, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.4s,\testimator xgboost's best error=0.8900,\tbest estimator lgbm's best error=0.7163\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.4s,\testimator xgboost's best error=0.8250,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 10, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.5s,\testimator extra_tree's best error=0.7758,\tbest estimator lgbm's best error=0.7712\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 11, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.4s,\testimator xgboost's best error=0.8739,\tbest estimator lgbm's best error=0.7497\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.5s,\testimator xgboost's best error=0.8157,\tbest estimator lgbm's best error=0.7163\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 11, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.5s,\testimator extra_tree's best error=0.8628,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 11, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.6s,\testimator extra_tree's best error=0.8533,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 11, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.6s,\testimator xgboost's best error=0.8159,\tbest estimator lgbm's best error=0.7712\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 12, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.5s,\testimator xgboost's best error=0.7767,\tbest estimator lgbm's best error=0.7497\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 11, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2392} INFO -  at 1.6s,\testimator extra_tree's best error=0.8219,\tbest estimator lgbm's best error=0.7163\n",
            "[flaml.automl.logger: 06-30 11:15:31] {2219} INFO - iteration 12, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 1.6s,\testimator xgboost's best error=0.7767,\tbest estimator lgbm's best error=0.7497\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 12, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 1.7s,\testimator extra_tree's best error=0.8465,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 12, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 1.7s,\testimator extra_tree's best error=0.8223,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 12, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 1.7s,\testimator extra_tree's best error=0.7758,\tbest estimator lgbm's best error=0.7712\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 13, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 1.7s,\testimator xgboost's best error=0.7767,\tbest estimator lgbm's best error=0.7497\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 13, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 1.7s,\testimator extra_tree's best error=0.8054,\tbest estimator lgbm's best error=0.7163\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 13, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 1.8s,\testimator extra_tree's best error=0.8223,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 13, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 1.8s,\testimator xgboost's best error=0.7767,\tbest estimator lgbm's best error=0.7497\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 14, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 1.9s,\testimator extra_tree's best error=0.7723,\tbest estimator lgbm's best error=0.7712\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 14, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 1.9s,\testimator rf's best error=0.8192,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 13, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 1.9s,\testimator xgboost's best error=0.8645,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 14, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.0s,\testimator rf's best error=0.7970,\tbest estimator lgbm's best error=0.7163\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 14, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.0s,\testimator extra_tree's best error=0.7723,\tbest estimator lgbm's best error=0.7712\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 15, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.1s,\testimator rf's best error=0.8192,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 14, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.1s,\testimator extra_tree's best error=0.8223,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 15, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.1s,\testimator lgbm's best error=0.7258,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 15, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.2s,\testimator lgbm's best error=0.7712,\tbest estimator lgbm's best error=0.7712\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 16, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.2s,\testimator rf's best error=0.7970,\tbest estimator lgbm's best error=0.7163\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 15, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.3s,\testimator extra_tree's best error=0.7978,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 16, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.4s,\testimator extra_tree's best error=0.7676,\tbest estimator extra_tree's best error=0.7676\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 17, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.4s,\testimator rf's best error=0.8866,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 16, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.4s,\testimator rf's best error=0.8192,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 15, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.5s,\testimator lgbm's best error=0.8095,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 17, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.5s,\testimator extra_tree's best error=0.7949,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 17, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.5s,\testimator xgboost's best error=0.8250,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 16, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2392} INFO -  at 2.5s,\testimator rf's best error=0.7970,\tbest estimator lgbm's best error=0.7163\n",
            "[flaml.automl.logger: 06-30 11:15:32] {2219} INFO - iteration 16, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 2.7s,\testimator rf's best error=0.8182,\tbest estimator extra_tree's best error=0.7676\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 18, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 2.7s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 17, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 2.7s,\testimator extra_tree's best error=0.8223,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 18, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 2.7s,\testimator rf's best error=0.8309,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 18, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 2.8s,\testimator extra_tree's best error=0.8465,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 18, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 2.8s,\testimator lgbm's best error=0.7258,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 19, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 2.9s,\testimator lgbm's best error=0.7608,\tbest estimator lgbm's best error=0.7608\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 19, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 2.9s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 17, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.0s,\testimator xgboost's best error=0.8159,\tbest estimator lgbm's best error=0.7608\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 20, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.0s,\testimator xgboost's best error=0.8157,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 18, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.0s,\testimator lgbm's best error=0.8095,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 19, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.1s,\testimator rf's best error=0.8309,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 20, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.1s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 19, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.1s,\testimator xgboost's best error=0.8645,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 20, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.2s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 19, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.2s,\testimator xgboost's best error=0.8250,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 20, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.3s,\testimator xgboost's best error=0.8157,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 20, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.4s,\testimator rf's best error=0.8182,\tbest estimator lgbm's best error=0.7608\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 21, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.5s,\testimator rf's best error=0.8866,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 21, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.5s,\testimator rf's best error=0.8309,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 21, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2392} INFO -  at 3.5s,\testimator rf's best error=0.8192,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:33] {2219} INFO - iteration 21, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2392} INFO -  at 3.6s,\testimator rf's best error=0.7970,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2219} INFO - iteration 21, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2392} INFO -  at 3.7s,\testimator rf's best error=0.8055,\tbest estimator lgbm's best error=0.7608\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2219} INFO - iteration 22, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2392} INFO -  at 3.8s,\testimator rf's best error=0.8309,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2219} INFO - iteration 22, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2392} INFO -  at 3.8s,\testimator rf's best error=0.8753,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2219} INFO - iteration 22, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2392} INFO -  at 3.8s,\testimator lgbm's best error=0.7608,\tbest estimator lgbm's best error=0.7608\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2219} INFO - iteration 23, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2392} INFO -  at 3.8s,\testimator rf's best error=0.8092,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2219} INFO - iteration 22, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2392} INFO -  at 3.9s,\testimator lgbm's best error=0.8095,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2219} INFO - iteration 23, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2392} INFO -  at 4.0s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2219} INFO - iteration 23, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2392} INFO -  at 4.0s,\testimator rf's best error=0.7924,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:34] {2219} INFO - iteration 22, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 4.8s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 24, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 4.9s,\testimator xgboost's best error=0.8250,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 4.9s,\testimator lgbm's best error=0.8095,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 24, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 5.1s,\testimator extra_tree's best error=0.8465,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 26, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 5.1s,\testimator extra_tree's best error=0.8223,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 5.2s,\testimator lgbm's best error=0.7608,\tbest estimator lgbm's best error=0.7608\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 24, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 5.2s,\testimator extra_tree's best error=0.8223,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 26, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 5.3s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 27, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 5.3s,\testimator extra_tree's best error=0.7676,\tbest estimator lgbm's best error=0.7608\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 5.3s,\testimator xgboost's best error=0.8645,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 27, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 5.4s,\testimator xgboost's best error=0.8250,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 28, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 5.5s,\testimator extra_tree's best error=0.7676,\tbest estimator lgbm's best error=0.7608\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 26, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 5.5s,\testimator extra_tree's best error=0.8164,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 28, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2392} INFO -  at 5.5s,\testimator xgboost's best error=0.7812,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:35] {2219} INFO - iteration 29, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 5.7s,\testimator xgboost's best error=0.7812,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 5.7s,\testimator extra_tree's best error=0.8164,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 29, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 5.8s,\testimator rf's best error=0.8055,\tbest estimator lgbm's best error=0.7608\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 27, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 5.8s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 5.9s,\testimator extra_tree's best error=0.8164,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 5.9s,\testimator xgboost's best error=0.8159,\tbest estimator lgbm's best error=0.7608\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 28, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 6.1s,\testimator lgbm's best error=0.8095,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 6.1s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 32, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 6.2s,\testimator lgbm's best error=0.8095,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 32, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 6.2s,\testimator extra_tree's best error=0.7676,\tbest estimator lgbm's best error=0.7608\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 29, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 6.3s,\testimator extra_tree's best error=0.7552,\tbest estimator extra_tree's best error=0.7552\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 6.3s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 33, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 6.3s,\testimator lgbm's best error=0.8095,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 33, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 6.5s,\testimator xgboost's best error=0.7812,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 34, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2392} INFO -  at 6.5s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7552\n",
            "[flaml.automl.logger: 06-30 11:15:36] {2219} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 6.7s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7552\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 32, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 6.8s,\testimator lgbm's best error=0.7258,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 23, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 6.8s,\testimator extra_tree's best error=0.8164,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 34, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 6.8s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 23, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 6.9s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 35, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 6.9s,\testimator xgboost's best error=0.8157,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 24, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.1s,\testimator rf's best error=0.7937,\tbest estimator extra_tree's best error=0.7552\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 33, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.1s,\testimator xgboost's best error=0.7812,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 36, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.1s,\testimator lgbm's best error=0.8095,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 35, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.1s,\testimator rf's best error=0.8270,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 24, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.1s,\testimator extra_tree's best error=0.8054,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.2s,\testimator xgboost's best error=0.8645,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 36, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.2s,\testimator xgboost's best error=0.7767,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.2s,\testimator extra_tree's best error=0.8465,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 37, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.3s,\testimator extra_tree's best error=0.7552,\tbest estimator extra_tree's best error=0.7552\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 34, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.3s,\testimator extra_tree's best error=0.7952,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 26, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.3s,\testimator extra_tree's best error=0.8164,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 37, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.3s,\testimator xgboost's best error=0.7812,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 38, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.3s,\testimator extra_tree's best error=0.7755,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 26, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.4s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7552\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 35, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.4s,\testimator xgboost's best error=0.8187,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 38, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.4s,\testimator xgboost's best error=0.7481,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 27, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.4s,\testimator xgboost's best error=0.7460,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 27, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.4s,\testimator xgboost's best error=0.7812,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 39, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.5s,\testimator xgboost's best error=0.8159,\tbest estimator extra_tree's best error=0.7552\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 36, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.5s,\testimator xgboost's best error=0.8187,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 39, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.5s,\testimator xgboost's best error=0.7812,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 40, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.5s,\testimator xgboost's best error=0.7481,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 28, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.6s,\testimator extra_tree's best error=0.7754,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 28, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2392} INFO -  at 7.6s,\testimator xgboost's best error=0.8187,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:37] {2219} INFO - iteration 40, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 7.6s,\testimator xgboost's best error=0.7481,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 29, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 7.7s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 41, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 7.7s,\testimator extra_tree's best error=0.7552,\tbest estimator extra_tree's best error=0.7552\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 37, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 7.7s,\testimator xgboost's best error=0.7460,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 29, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 7.7s,\testimator xgboost's best error=0.8187,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 41, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 7.7s,\testimator xgboost's best error=0.7481,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 7.8s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 42, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 7.8s,\testimator xgboost's best error=0.7460,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 7.8s,\testimator xgboost's best error=0.8187,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 42, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 7.8s,\testimator extra_tree's best error=0.7552,\tbest estimator extra_tree's best error=0.7552\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 38, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 7.9s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 43, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.0s,\testimator extra_tree's best error=0.7552,\tbest estimator extra_tree's best error=0.7552\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 39, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.1s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 44, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.1s,\testimator xgboost's best error=0.8159,\tbest estimator extra_tree's best error=0.7552\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 40, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.2s,\testimator lgbm's best error=0.8095,\tbest estimator lgbm's best error=0.8095\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 43, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.2s,\testimator lgbm's best error=0.7258,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.3s,\testimator xgboost's best error=0.8044,\tbest estimator xgboost's best error=0.8044\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 44, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.2s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.3s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.3s,\testimator extra_tree's best error=0.8465,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 41, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 45, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.5s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 46, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.5s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 42, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2392} INFO -  at 8.5s,\testimator lgbm's best error=0.7258,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:38] {2219} INFO - iteration 32, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 8.7s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 32, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 8.9s,\testimator extra_tree's best error=0.8164,\tbest estimator xgboost's best error=0.8044\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 45, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 8.9s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 47, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.0s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 43, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.0s,\testimator rf's best error=0.7924,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 33, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.1s,\testimator xgboost's best error=0.8044,\tbest estimator xgboost's best error=0.8044\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 46, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.1s,\testimator lgbm's best error=0.7258,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 33, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.1s,\testimator xgboost's best error=0.7481,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 34, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.2s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 48, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.2s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 44, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.2s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 47, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.2s,\testimator xgboost's best error=0.7460,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 34, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.3s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 49, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.3s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 45, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.4s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 48, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.4s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 50, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.5s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 46, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.5s,\testimator extra_tree's best error=0.8465,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 51, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.6s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 49, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.6s,\testimator xgboost's best error=0.8159,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 47, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2392} INFO -  at 9.6s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:39] {2219} INFO - iteration 35, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 9.6s,\testimator lgbm's best error=0.7641,\tbest estimator lgbm's best error=0.7641\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 52, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 9.7s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 50, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 9.7s,\testimator xgboost's best error=0.7481,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 36, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 9.7s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 48, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 9.9s,\testimator extra_tree's best error=0.7952,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 37, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 9.9s,\testimator lgbm's best error=0.7258,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 35, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.0s,\testimator lgbm's best error=0.7576,\tbest estimator lgbm's best error=0.7576\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 53, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.0s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 51, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.0s,\testimator rf's best error=0.7937,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 49, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.0s,\testimator xgboost's best error=0.7460,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 36, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.1s,\testimator xgboost's best error=0.7360,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 38, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.1s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 52, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.2s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 50, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.2s,\testimator xgboost's best error=0.7460,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 37, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.2s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 54, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.2s,\testimator xgboost's best error=0.7360,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 39, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.3s,\testimator xgboost's best error=0.7460,\tbest estimator lgbm's best error=0.7258\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 38, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.3s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 51, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.4s,\testimator xgboost's best error=0.8071,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 52, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.4s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 39, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.5s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 55, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.5s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 40, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.6s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 53, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2392} INFO -  at 10.6s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:40] {2219} INFO - iteration 53, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 10.7s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 41, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 10.8s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 40, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 10.8s,\testimator lgbm's best error=0.8095,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 54, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 10.8s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 42, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 10.9s,\testimator rf's best error=0.7888,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 54, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 11.1s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 56, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 11.1s,\testimator lgbm's best error=0.8095,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 55, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 11.1s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 41, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 11.2s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 55, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 11.3s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 56, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 11.2s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 42, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 11.4s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 56, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 11.6s,\testimator xgboost's best error=0.8071,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 57, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 11.6s,\testimator lgbm's best error=0.8040,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 57, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2392} INFO -  at 11.6s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:41] {2219} INFO - iteration 57, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 11.7s,\testimator lgbm's best error=0.7258,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 43, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 11.7s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 58, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 11.8s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 58, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 11.8s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 59, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 11.8s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 44, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 11.9s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 59, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 12.0s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 45, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 12.1s,\testimator lgbm's best error=0.8040,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 12.1s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 60, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 60, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 12.1s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 43, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 12.2s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 61, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 12.2s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 44, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 12.4s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 58, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 12.5s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 45, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 12.5s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 61, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2392} INFO -  at 12.5s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:42] {2219} INFO - iteration 46, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 12.7s,\testimator rf's best error=0.7790,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 62, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 12.8s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 46, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 12.8s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 63, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 12.8s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 47, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 12.9s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 59, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.0s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 64, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.0s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 47, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.0s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 48, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.1s,\testimator lgbm's best error=0.8040,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 62, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.1s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 48, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.2s,\testimator rf's best error=0.7790,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 65, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.3s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 63, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.4s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 66, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.5s,\testimator lgbm's best error=0.7258,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 49, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.5s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 49, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.6s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 67, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.6s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 50, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2392} INFO -  at 13.6s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:43] {2219} INFO - iteration 60, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 13.7s,\testimator xgboost's best error=0.7939,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 68, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 13.8s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 51, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 13.8s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 50, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 13.8s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 64, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 13.9s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 69, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 13.9s,\testimator rf's best error=0.8092,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 61, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 13.9s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 52, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 13.9s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 51, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.0s,\testimator xgboost's best error=0.7939,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 70, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.1s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 62, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.1s,\testimator lgbm's best error=0.8040,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 65, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.1s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 71, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.2s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 66, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.2s,\testimator xgboost's best error=0.7939,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 72, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.2s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 53, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.4s,\testimator rf's best error=0.8092,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 63, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.5s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 73, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.5s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 67, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.5s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 52, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.6s,\testimator extra_tree's best error=0.7372,\tbest estimator extra_tree's best error=0.7372\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 74, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2392} INFO -  at 14.6s,\testimator xgboost's best error=0.7980,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:44] {2219} INFO - iteration 68, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2392} INFO -  at 14.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2219} INFO - iteration 75, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2392} INFO -  at 14.8s,\testimator lgbm's best error=0.7258,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2219} INFO - iteration 54, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2392} INFO -  at 14.8s,\testimator lgbm's best error=0.8040,\tbest estimator xgboost's best error=0.7980\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2219} INFO - iteration 69, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2392} INFO -  at 14.9s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2219} INFO - iteration 64, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2392} INFO -  at 15.0s,\testimator rf's best error=0.7790,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2219} INFO - iteration 76, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2392} INFO -  at 15.0s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2219} INFO - iteration 53, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2392} INFO -  at 15.3s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2219} INFO - iteration 65, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2392} INFO -  at 15.3s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2219} INFO - iteration 54, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2392} INFO -  at 15.4s,\testimator xgboost's best error=0.7856,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2219} INFO - iteration 70, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2392} INFO -  at 15.4s,\testimator lgbm's best error=0.7258,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:45] {2219} INFO - iteration 55, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 15.7s,\testimator rf's best error=0.7740,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 77, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 15.6s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 56, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 15.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 78, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.0s,\testimator xgboost's best error=0.7650,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 79, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 15.9s,\testimator rf's best error=0.8270,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 57, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.1s,\testimator xgboost's best error=0.7650,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 80, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.1s,\testimator xgboost's best error=0.7856,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 71, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.1s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 58, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.1s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 55, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 81, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.2s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 59, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.3s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 66, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.4s,\testimator xgboost's best error=0.7650,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 82, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.3s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 56, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.4s,\testimator lgbm's best error=0.7258,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 60, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.5s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 67, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2392} INFO -  at 16.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:46] {2219} INFO - iteration 83, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 16.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 84, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 16.7s,\testimator xgboost's best error=0.7856,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 72, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 16.7s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 61, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 16.7s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 68, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 16.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 85, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 16.8s,\testimator extra_tree's best error=0.8164,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 73, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 16.8s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 57, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 16.9s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 86, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 17.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 87, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 17.2s,\testimator rf's best error=0.7884,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 58, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 17.3s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 88, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 17.4s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 59, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 17.4s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 89, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 17.5s,\testimator xgboost's best error=0.7856,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 74, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 17.5s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 69, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2392} INFO -  at 17.6s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:47] {2219} INFO - iteration 90, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 17.6s,\testimator lgbm's best error=0.7248,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 62, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 17.6s,\testimator lgbm's best error=0.7089,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 60, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 17.7s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 70, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 17.7s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 91, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 17.8s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 63, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 17.8s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 71, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 17.8s,\testimator xgboost's best error=0.7856,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 75, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 17.9s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 92, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 17.9s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 64, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 18.0s,\testimator rf's best error=0.8092,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 72, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 18.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 93, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 18.1s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.7089\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 61, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 18.1s,\testimator rf's best error=0.8753,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 76, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 18.2s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 65, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 18.3s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 94, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 18.3s,\testimator extra_tree's best error=0.8459,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 73, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 18.3s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 66, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2392} INFO -  at 18.5s,\testimator rf's best error=0.8537,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:48] {2219} INFO - iteration 77, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 18.7s,\testimator rf's best error=0.8092,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 74, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 18.7s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 62, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 18.8s,\testimator rf's best error=0.7740,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 95, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 18.7s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 67, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 18.9s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 75, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 18.8s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 68, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 18.8s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 63, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 18.9s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 96, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 19.0s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 64, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 19.0s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 97, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 19.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 98, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 19.2s,\testimator rf's best error=0.8044,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 69, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 19.3s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 76, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 19.3s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 99, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 19.3s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 70, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 19.4s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 100, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 19.5s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 71, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2392} INFO -  at 19.6s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:49] {2219} INFO - iteration 101, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 19.6s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 72, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 19.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 102, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 19.8s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 77, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 19.7s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 65, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 19.8s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 73, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 19.9s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 78, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 19.9s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 103, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 19.9s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 74, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 20.0s,\testimator rf's best error=0.7884,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 66, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 20.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 104, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 20.1s,\testimator xgboost's best error=0.7856,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 78, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 20.1s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 75, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 20.2s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 105, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 20.2s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 67, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 20.4s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 106, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 20.5s,\testimator rf's best error=0.8514,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 79, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 20.4s,\testimator rf's best error=0.8044,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 76, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 20.6s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 107, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2392} INFO -  at 20.6s,\testimator rf's best error=0.7884,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:50] {2219} INFO - iteration 68, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 20.8s,\testimator rf's best error=0.8514,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 80, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 20.8s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 108, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 20.8s,\testimator rf's best error=0.8044,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 77, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 21.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 109, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 21.1s,\testimator extra_tree's best error=0.8164,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 81, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 21.1s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 78, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 21.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 110, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 21.3s,\testimator xgboost's best error=0.7856,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 82, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 21.3s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 79, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 21.3s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 111, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 21.4s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 79, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 21.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 112, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2392} INFO -  at 21.6s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:51] {2219} INFO - iteration 80, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2392} INFO -  at 21.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2219} INFO - iteration 113, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2392} INFO -  at 21.7s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2219} INFO - iteration 69, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2392} INFO -  at 21.8s,\testimator rf's best error=0.7735,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2219} INFO - iteration 80, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2392} INFO -  at 21.9s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2219} INFO - iteration 114, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2392} INFO -  at 22.0s,\testimator xgboost's best error=0.7856,\tbest estimator xgboost's best error=0.7856\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2219} INFO - iteration 83, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2392} INFO -  at 22.0s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2219} INFO - iteration 81, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2392} INFO -  at 22.0s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2219} INFO - iteration 81, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2392} INFO -  at 22.0s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2219} INFO - iteration 115, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2392} INFO -  at 22.3s,\testimator rf's best error=0.7735,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2219} INFO - iteration 82, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2392} INFO -  at 22.5s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2219} INFO - iteration 82, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2392} INFO -  at 22.5s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:52] {2219} INFO - iteration 83, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2392} INFO -  at 22.7s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2219} INFO - iteration 83, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2392} INFO -  at 22.7s,\testimator xgboost's best error=0.7108,\tbest estimator xgboost's best error=0.7108\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2219} INFO - iteration 84, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2392} INFO -  at 22.7s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2219} INFO - iteration 70, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2392} INFO -  at 22.8s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2219} INFO - iteration 84, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2392} INFO -  at 22.9s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2219} INFO - iteration 84, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2392} INFO -  at 23.0s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2219} INFO - iteration 85, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2392} INFO -  at 23.0s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2219} INFO - iteration 71, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2392} INFO -  at 23.1s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2219} INFO - iteration 85, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2392} INFO -  at 23.3s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2219} INFO - iteration 72, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2392} INFO -  at 23.5s,\testimator rf's best error=0.8092,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:53] {2219} INFO - iteration 86, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 23.6s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 73, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 23.6s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 86, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 23.8s,\testimator rf's best error=0.7683,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 116, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 23.8s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 87, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 23.9s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 87, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 23.9s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 74, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 24.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 117, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 24.0s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 85, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 24.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 118, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 24.1s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 75, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 24.2s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 88, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2392} INFO -  at 24.6s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:54] {2219} INFO - iteration 119, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2392} INFO -  at 24.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2219} INFO - iteration 120, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2392} INFO -  at 24.7s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2219} INFO - iteration 76, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2392} INFO -  at 25.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2219} INFO - iteration 121, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2392} INFO -  at 25.0s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2219} INFO - iteration 86, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2392} INFO -  at 25.1s,\testimator rf's best error=0.7729,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2219} INFO - iteration 88, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2392} INFO -  at 25.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2219} INFO - iteration 122, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2392} INFO -  at 25.3s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2219} INFO - iteration 123, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2392} INFO -  at 25.3s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2219} INFO - iteration 87, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2392} INFO -  at 25.3s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2219} INFO - iteration 89, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2392} INFO -  at 25.5s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:55] {2219} INFO - iteration 90, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2392} INFO -  at 25.8s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2219} INFO - iteration 77, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2392} INFO -  at 25.9s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2219} INFO - iteration 91, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2392} INFO -  at 26.0s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2219} INFO - iteration 78, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2392} INFO -  at 26.2s,\testimator rf's best error=0.7683,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2219} INFO - iteration 124, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2392} INFO -  at 26.2s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2219} INFO - iteration 79, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2392} INFO -  at 26.3s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2219} INFO - iteration 92, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2392} INFO -  at 26.4s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2219} INFO - iteration 125, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2392} INFO -  at 26.5s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2219} INFO - iteration 93, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2392} INFO -  at 26.5s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2219} INFO - iteration 80, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2392} INFO -  at 26.6s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:56] {2219} INFO - iteration 126, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 26.7s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 94, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 26.7s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 81, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 26.8s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 89, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 26.8s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 127, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 26.9s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 128, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.0s,\testimator xgb_limitdepth's best error=0.8813,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 90, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 129, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.0s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 95, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.1s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 88, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.2s,\testimator xgb_limitdepth's best error=0.7908,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 91, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.2s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 96, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 130, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.3s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 97, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.4s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 131, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.4s,\testimator xgb_limitdepth's best error=0.7908,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 92, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 132, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.5s,\testimator xgb_limitdepth's best error=0.7908,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 93, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2392} INFO -  at 27.5s,\testimator lgbm's best error=0.7036,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:57] {2219} INFO - iteration 82, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2392} INFO -  at 27.6s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2219} INFO - iteration 98, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2392} INFO -  at 27.7s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2219} INFO - iteration 83, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2392} INFO -  at 28.0s,\testimator xgb_limitdepth's best error=0.7908,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2219} INFO - iteration 94, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2392} INFO -  at 28.0s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2219} INFO - iteration 84, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2392} INFO -  at 28.0s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2219} INFO - iteration 89, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2392} INFO -  at 28.2s,\testimator extra_tree's best error=0.8164,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2219} INFO - iteration 90, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2392} INFO -  at 28.2s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2219} INFO - iteration 85, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2392} INFO -  at 28.2s,\testimator xgb_limitdepth's best error=0.7908,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2219} INFO - iteration 95, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2392} INFO -  at 28.3s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2219} INFO - iteration 99, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2392} INFO -  at 28.5s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2219} INFO - iteration 96, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2392} INFO -  at 28.5s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:58] {2219} INFO - iteration 100, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2392} INFO -  at 28.7s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2219} INFO - iteration 86, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2392} INFO -  at 28.8s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2219} INFO - iteration 101, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2392} INFO -  at 28.8s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2219} INFO - iteration 91, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2392} INFO -  at 28.9s,\testimator xgb_limitdepth's best error=0.7908,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2219} INFO - iteration 97, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2392} INFO -  at 28.8s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.7036\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2219} INFO - iteration 87, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2392} INFO -  at 28.9s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2219} INFO - iteration 102, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2392} INFO -  at 29.4s,\testimator xgb_limitdepth's best error=0.7863,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2219} INFO - iteration 98, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2392} INFO -  at 29.3s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2219} INFO - iteration 88, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2392} INFO -  at 29.6s,\testimator extra_tree's best error=0.8164,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:15:59] {2219} INFO - iteration 92, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2392} INFO -  at 29.7s,\testimator xgboost's best error=0.7717,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2219} INFO - iteration 99, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2392} INFO -  at 29.7s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2219} INFO - iteration 89, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2392} INFO -  at 29.8s,\testimator lgbm's best error=0.7153,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2219} INFO - iteration 103, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2392} INFO -  at 29.9s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2219} INFO - iteration 90, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2392} INFO -  at 30.0s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2219} INFO - iteration 100, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2392} INFO -  at 30.1s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2219} INFO - iteration 101, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2392} INFO -  at 30.2s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2219} INFO - iteration 91, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2392} INFO -  at 30.4s,\testimator xgb_limitdepth's best error=0.7863,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2219} INFO - iteration 102, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2392} INFO -  at 30.4s,\testimator extra_tree's best error=0.7686,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2219} INFO - iteration 92, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2392} INFO -  at 30.6s,\testimator lgbm's best error=0.7153,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:00] {2219} INFO - iteration 104, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:01] {2392} INFO -  at 30.7s,\testimator rf's best error=0.8092,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:01] {2219} INFO - iteration 103, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:01] {2392} INFO -  at 30.8s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:01] {2219} INFO - iteration 93, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 31.7s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 93, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 31.7s,\testimator lgbm's best error=0.7078,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 105, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 31.8s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 104, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 32.0s,\testimator rf's best error=0.7683,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 133, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 32.0s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 105, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 32.0s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 94, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 32.1s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 134, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 32.1s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 95, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 32.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 135, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 32.3s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 94, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 32.3s,\testimator extra_tree's best error=0.7647,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 96, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 32.4s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 136, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 32.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 137, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2392} INFO -  at 32.6s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:02] {2219} INFO - iteration 95, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 32.9s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 138, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 32.9s,\testimator extra_tree's best error=0.8154,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 96, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 33.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 139, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 33.1s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 106, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 33.1s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 97, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 33.2s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 140, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 33.2s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 107, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 33.3s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 141, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 33.3s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 97, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 33.4s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 98, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 33.5s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 142, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2392} INFO -  at 33.5s,\testimator xgb_limitdepth's best error=0.7711,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:03] {2219} INFO - iteration 108, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2392} INFO -  at 33.6s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2219} INFO - iteration 143, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2392} INFO -  at 33.8s,\testimator xgb_limitdepth's best error=0.7711,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2219} INFO - iteration 109, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2392} INFO -  at 33.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2219} INFO - iteration 144, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2392} INFO -  at 33.9s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2219} INFO - iteration 98, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2392} INFO -  at 33.9s,\testimator lgbm's best error=0.7078,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2219} INFO - iteration 106, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2392} INFO -  at 34.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2219} INFO - iteration 145, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2392} INFO -  at 34.1s,\testimator xgb_limitdepth's best error=0.7711,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2219} INFO - iteration 110, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2392} INFO -  at 34.2s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2219} INFO - iteration 107, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2392} INFO -  at 34.3s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:04] {2219} INFO - iteration 99, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2392} INFO -  at 34.9s,\testimator rf's best error=0.7683,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2219} INFO - iteration 146, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2392} INFO -  at 35.0s,\testimator xgb_limitdepth's best error=0.7621,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2219} INFO - iteration 111, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2392} INFO -  at 35.0s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2219} INFO - iteration 100, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2392} INFO -  at 35.2s,\testimator lgbm's best error=0.7608,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2219} INFO - iteration 147, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2392} INFO -  at 35.3s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2219} INFO - iteration 112, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2392} INFO -  at 35.6s,\testimator xgb_limitdepth's best error=0.7621,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:05] {2219} INFO - iteration 113, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:06] {2392} INFO -  at 36.0s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:06] {2219} INFO - iteration 99, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:06] {2392} INFO -  at 36.1s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:06] {2219} INFO - iteration 100, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:06] {2392} INFO -  at 36.4s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:06] {2219} INFO - iteration 101, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:06] {2392} INFO -  at 36.5s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:06] {2219} INFO - iteration 101, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2392} INFO -  at 37.0s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2219} INFO - iteration 102, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2392} INFO -  at 37.1s,\testimator rf's best error=0.8455,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2219} INFO - iteration 102, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2392} INFO -  at 37.2s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2219} INFO - iteration 114, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2392} INFO -  at 37.3s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2219} INFO - iteration 103, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2392} INFO -  at 37.3s,\testimator lgbm's best error=0.7078,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2219} INFO - iteration 108, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2392} INFO -  at 37.6s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:07] {2219} INFO - iteration 115, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:08] {2392} INFO -  at 38.0s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:08] {2219} INFO - iteration 103, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:08] {2392} INFO -  at 38.2s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:08] {2219} INFO - iteration 109, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:08] {2392} INFO -  at 38.4s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:08] {2219} INFO - iteration 116, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:08] {2392} INFO -  at 38.6s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:08] {2219} INFO - iteration 104, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2392} INFO -  at 38.6s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2219} INFO - iteration 110, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2392} INFO -  at 38.7s,\testimator rf's best error=0.8092,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2219} INFO - iteration 117, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2392} INFO -  at 38.7s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2219} INFO - iteration 105, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2392} INFO -  at 38.8s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2219} INFO - iteration 106, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2392} INFO -  at 38.9s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2219} INFO - iteration 104, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2392} INFO -  at 39.2s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2219} INFO - iteration 105, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2392} INFO -  at 39.5s,\testimator rf's best error=0.8455,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:09] {2219} INFO - iteration 106, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2392} INFO -  at 39.8s,\testimator lgbm's best error=0.7078,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2219} INFO - iteration 111, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2392} INFO -  at 40.1s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2219} INFO - iteration 112, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2392} INFO -  at 40.2s,\testimator rf's best error=0.7683,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2219} INFO - iteration 148, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2392} INFO -  at 40.2s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2219} INFO - iteration 107, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2392} INFO -  at 40.3s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2219} INFO - iteration 118, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2392} INFO -  at 40.5s,\testimator lgbm's best error=0.7523,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:10] {2219} INFO - iteration 149, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2392} INFO -  at 40.6s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2219} INFO - iteration 113, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2392} INFO -  at 40.7s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2219} INFO - iteration 119, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2392} INFO -  at 40.8s,\testimator extra_tree's best error=0.7754,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2219} INFO - iteration 114, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2392} INFO -  at 40.9s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2219} INFO - iteration 120, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2392} INFO -  at 41.2s,\testimator lgbm's best error=0.7523,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2219} INFO - iteration 150, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2392} INFO -  at 41.4s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2219} INFO - iteration 107, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2392} INFO -  at 41.4s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2219} INFO - iteration 151, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2392} INFO -  at 41.6s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:11] {2219} INFO - iteration 152, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 41.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 153, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.0s,\testimator lgbm's best error=0.7523,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 154, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.0s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 108, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.0s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 108, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.1s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 121, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.1s,\testimator lgbm's best error=0.7523,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 155, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.1s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 109, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.2s,\testimator xgb_limitdepth's best error=0.9022,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 109, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.3s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 156, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.3s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 110, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.4s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 122, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.4s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 157, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.4s,\testimator xgb_limitdepth's best error=0.8247,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 110, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2392} INFO -  at 42.5s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:12] {2219} INFO - iteration 158, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 42.6s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 159, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 42.7s,\testimator xgb_limitdepth's best error=0.8247,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 111, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 42.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 160, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 42.9s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 112, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 42.9s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 161, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 43.0s,\testimator xgb_limitdepth's best error=0.8028,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 113, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 43.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 162, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 43.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 163, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 43.2s,\testimator xgb_limitdepth's best error=0.8028,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 114, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 43.3s,\testimator xgb_limitdepth's best error=0.8028,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 115, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 43.3s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 164, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 43.5s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 165, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2392} INFO -  at 43.6s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:13] {2219} INFO - iteration 123, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2392} INFO -  at 43.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2219} INFO - iteration 166, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2392} INFO -  at 43.8s,\testimator lgbm's best error=0.7078,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2219} INFO - iteration 115, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2392} INFO -  at 43.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2219} INFO - iteration 167, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2392} INFO -  at 43.9s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2219} INFO - iteration 116, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2392} INFO -  at 44.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2219} INFO - iteration 168, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2392} INFO -  at 44.0s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2219} INFO - iteration 111, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2392} INFO -  at 44.0s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2219} INFO - iteration 124, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2392} INFO -  at 44.4s,\testimator xgb_limitdepth's best error=0.8251,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2219} INFO - iteration 112, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2392} INFO -  at 44.4s,\testimator lgbm's best error=0.7078,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:14] {2219} INFO - iteration 116, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2392} INFO -  at 44.6s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2219} INFO - iteration 117, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2392} INFO -  at 44.7s,\testimator xgb_limitdepth's best error=0.7565,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2219} INFO - iteration 113, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2392} INFO -  at 45.1s,\testimator xgb_limitdepth's best error=0.7565,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2219} INFO - iteration 114, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2392} INFO -  at 45.1s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2219} INFO - iteration 118, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2392} INFO -  at 45.3s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2219} INFO - iteration 125, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2392} INFO -  at 45.5s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2219} INFO - iteration 126, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2392} INFO -  at 45.4s,\testimator xgb_limitdepth's best error=0.7519,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2219} INFO - iteration 115, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2392} INFO -  at 45.6s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:15] {2219} INFO - iteration 116, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:16] {2392} INFO -  at 45.9s,\testimator lgbm's best error=0.7078,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:16] {2219} INFO - iteration 119, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:16] {2392} INFO -  at 46.2s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:16] {2219} INFO - iteration 117, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:16] {2392} INFO -  at 46.3s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:16] {2219} INFO - iteration 120, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2392} INFO -  at 46.9s,\testimator xgb_limitdepth's best error=0.8028,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2219} INFO - iteration 118, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2392} INFO -  at 46.9s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2219} INFO - iteration 121, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2392} INFO -  at 47.1s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2219} INFO - iteration 127, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2392} INFO -  at 47.2s,\testimator xgb_limitdepth's best error=0.8028,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2219} INFO - iteration 119, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2392} INFO -  at 47.2s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2219} INFO - iteration 122, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2392} INFO -  at 47.3s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2219} INFO - iteration 128, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2392} INFO -  at 47.5s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2219} INFO - iteration 129, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2392} INFO -  at 47.6s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:17] {2219} INFO - iteration 120, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2392} INFO -  at 47.7s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2219} INFO - iteration 123, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2392} INFO -  at 47.8s,\testimator xgb_limitdepth's best error=0.8028,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2219} INFO - iteration 121, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2392} INFO -  at 47.9s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2219} INFO - iteration 117, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2392} INFO -  at 48.1s,\testimator xgb_limitdepth's best error=0.8028,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2219} INFO - iteration 122, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2392} INFO -  at 48.1s,\testimator xgb_limitdepth's best error=0.7519,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2219} INFO - iteration 118, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2392} INFO -  at 48.3s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2219} INFO - iteration 130, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2392} INFO -  at 48.3s,\testimator xgb_limitdepth's best error=0.8028,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2219} INFO - iteration 123, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2392} INFO -  at 48.3s,\testimator extra_tree's best error=0.7647,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2219} INFO - iteration 119, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2392} INFO -  at 48.5s,\testimator rf's best error=0.7683,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2219} INFO - iteration 169, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2392} INFO -  at 48.5s,\testimator xgb_limitdepth's best error=0.8028,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:18] {2219} INFO - iteration 124, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2392} INFO -  at 48.8s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2219} INFO - iteration 131, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2392} INFO -  at 48.8s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2219} INFO - iteration 125, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2392} INFO -  at 49.1s,\testimator lgbm's best error=0.7523,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2219} INFO - iteration 170, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2392} INFO -  at 49.2s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2219} INFO - iteration 126, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2392} INFO -  at 49.3s,\testimator lgbm's best error=0.7523,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2219} INFO - iteration 171, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2392} INFO -  at 49.3s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2219} INFO - iteration 120, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2392} INFO -  at 49.4s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2219} INFO - iteration 172, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2392} INFO -  at 49.4s,\testimator xgb_limitdepth's best error=0.7519,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2219} INFO - iteration 121, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2392} INFO -  at 49.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2219} INFO - iteration 173, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2392} INFO -  at 49.6s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:19] {2219} INFO - iteration 132, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 49.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 174, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 49.6s,\testimator xgb_limitdepth's best error=0.7519,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 122, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 49.8s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 175, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 49.8s,\testimator xgb_limitdepth's best error=0.7519,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 123, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 49.8s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 133, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 49.9s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 127, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 49.9s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 176, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 50.0s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 128, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 50.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 177, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 50.1s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 124, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 50.2s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 178, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 50.3s,\testimator rf's best error=0.8455,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 129, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 50.4s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 179, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 50.4s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 125, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 50.5s,\testimator xgb_limitdepth's best error=0.7986,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 130, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2392} INFO -  at 50.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:20] {2219} INFO - iteration 180, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2392} INFO -  at 50.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2219} INFO - iteration 181, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2392} INFO -  at 50.8s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2219} INFO - iteration 182, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2392} INFO -  at 50.9s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2219} INFO - iteration 131, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2392} INFO -  at 51.3s,\testimator lgbm's best error=0.7523,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2219} INFO - iteration 183, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2392} INFO -  at 51.4s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2219} INFO - iteration 184, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2392} INFO -  at 51.6s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2219} INFO - iteration 185, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2392} INFO -  at 51.6s,\testimator lgbm's best error=0.7078,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:21] {2219} INFO - iteration 124, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 51.7s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 186, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 51.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 187, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.0s,\testimator lgbm's best error=0.7523,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 188, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.0s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 126, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.1s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 189, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.1s,\testimator lgbm's best error=0.6935,\tbest estimator lgbm's best error=0.6935\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 127, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 190, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.2s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 132, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.4s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 191, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.4s,\testimator xgb_limitdepth's best error=0.7986,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 133, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 192, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.5s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 128, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.5s,\testimator lgbm's best error=0.7070,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 125, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2392} INFO -  at 52.6s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:22] {2219} INFO - iteration 134, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2392} INFO -  at 52.8s,\testimator rf's best error=0.8092,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2219} INFO - iteration 135, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2392} INFO -  at 52.8s,\testimator xgb_limitdepth's best error=0.9781,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2219} INFO - iteration 193, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2392} INFO -  at 52.8s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2219} INFO - iteration 134, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2392} INFO -  at 53.2s,\testimator xgb_limitdepth's best error=0.7956,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2219} INFO - iteration 194, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2392} INFO -  at 53.2s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2219} INFO - iteration 129, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2392} INFO -  at 53.3s,\testimator xgb_limitdepth's best error=0.7519,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2219} INFO - iteration 130, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2392} INFO -  at 53.5s,\testimator xgb_limitdepth's best error=0.7956,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2219} INFO - iteration 195, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2392} INFO -  at 53.5s,\testimator rf's best error=0.8299,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:23] {2219} INFO - iteration 135, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 53.7s,\testimator xgb_limitdepth's best error=0.7652,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 196, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 53.9s,\testimator rf's best error=0.8299,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 136, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 54.0s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 197, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 53.9s,\testimator rf's best error=0.7877,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 131, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 54.0s,\testimator lgbm's best error=0.7070,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 126, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 54.1s,\testimator xgb_limitdepth's best error=0.7943,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 137, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 54.2s,\testimator xgb_limitdepth's best error=0.7652,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 198, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 54.3s,\testimator xgb_limitdepth's best error=0.7652,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 199, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 54.3s,\testimator rf's best error=0.7841,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 132, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 54.3s,\testimator xgb_limitdepth's best error=0.7943,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 138, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 54.5s,\testimator xgb_limitdepth's best error=0.7652,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 200, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 54.5s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 139, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 54.5s,\testimator extra_tree's best error=0.7647,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 133, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2392} INFO -  at 54.6s,\testimator xgb_limitdepth's best error=0.7652,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:24] {2219} INFO - iteration 201, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2392} INFO -  at 54.8s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2219} INFO - iteration 136, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2392} INFO -  at 54.9s,\testimator xgb_limitdepth's best error=0.7943,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2219} INFO - iteration 140, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2392} INFO -  at 55.0s,\testimator lgbm's best error=0.7523,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2219} INFO - iteration 202, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2392} INFO -  at 55.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2219} INFO - iteration 203, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2392} INFO -  at 55.1s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2219} INFO - iteration 134, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2392} INFO -  at 55.3s,\testimator lgbm's best error=0.7070,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2219} INFO - iteration 127, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2392} INFO -  at 55.4s,\testimator xgb_limitdepth's best error=0.7652,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2219} INFO - iteration 204, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2392} INFO -  at 55.4s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2219} INFO - iteration 137, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2392} INFO -  at 55.5s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2219} INFO - iteration 128, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2392} INFO -  at 55.5s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2219} INFO - iteration 135, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2392} INFO -  at 55.5s,\testimator xgb_limitdepth's best error=0.7652,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:25] {2219} INFO - iteration 205, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2392} INFO -  at 55.7s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2219} INFO - iteration 138, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2392} INFO -  at 55.9s,\testimator xgb_limitdepth's best error=0.7652,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2219} INFO - iteration 206, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2392} INFO -  at 56.2s,\testimator rf's best error=0.7699,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2219} INFO - iteration 136, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2392} INFO -  at 56.2s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2219} INFO - iteration 129, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2392} INFO -  at 56.3s,\testimator xgb_limitdepth's best error=0.7652,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2219} INFO - iteration 207, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2392} INFO -  at 56.3s,\testimator xgb_limitdepth's best error=0.7519,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2219} INFO - iteration 137, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2392} INFO -  at 56.4s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:26] {2219} INFO - iteration 130, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 56.7s,\testimator xgb_limitdepth's best error=0.7652,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 208, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 56.8s,\testimator rf's best error=0.7699,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 138, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.0s,\testimator xgb_limitdepth's best error=0.7652,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 209, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.0s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 131, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.1s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 139, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.1s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 139, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 210, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.1s,\testimator rf's best error=0.8299,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 141, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.2s,\testimator xgboost's best error=0.7299,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 140, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.2s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 132, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.3s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 211, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.3s,\testimator xgb_limitdepth's best error=0.7943,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 142, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.3s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 133, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.4s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 212, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2392} INFO -  at 57.6s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:27] {2219} INFO - iteration 213, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 57.6s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 134, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 57.7s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 140, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 57.7s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 214, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 57.7s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 143, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 57.9s,\testimator xgb_limitdepth's best error=0.8183,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 135, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.0s,\testimator rf's best error=0.8092,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 141, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.0s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 215, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.0s,\testimator xgb_limitdepth's best error=0.7839,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 136, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.1s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 216, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.2s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 142, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.2s,\testimator xgb_limitdepth's best error=0.7900,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 144, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.3s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 217, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.3s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 141, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.3s,\testimator xgb_limitdepth's best error=0.7839,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 137, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.4s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 218, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.4s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 143, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.4s,\testimator xgb_limitdepth's best error=0.7900,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 145, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2392} INFO -  at 58.4s,\testimator xgb_limitdepth's best error=0.7628,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:28] {2219} INFO - iteration 138, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:29] {2392} INFO -  at 59.0s,\testimator xgboost's best error=0.7256,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:29] {2219} INFO - iteration 142, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:29] {2392} INFO -  at 59.0s,\testimator rf's best error=0.7694,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:29] {2219} INFO - iteration 139, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:29] {2392} INFO -  at 59.2s,\testimator lgbm's best error=0.7523,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:29] {2219} INFO - iteration 219, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:29] {2392} INFO -  at 59.5s,\testimator xgb_limitdepth's best error=0.7628,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:29] {2219} INFO - iteration 140, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2392} INFO -  at 60.1s,\testimator rf's best error=0.7683,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2219} INFO - iteration 220, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2392} INFO -  at 60.2s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2219} INFO - iteration 141, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2392} INFO -  at 60.3s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2219} INFO - iteration 221, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2392} INFO -  at 60.3s,\testimator lgbm's best error=0.7523,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2219} INFO - iteration 222, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2392} INFO -  at 60.4s,\testimator xgb_limitdepth's best error=0.7628,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2219} INFO - iteration 142, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2392} INFO -  at 60.4s,\testimator rf's best error=0.7699,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2219} INFO - iteration 143, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2392} INFO -  at 60.5s,\testimator xgb_limitdepth's best error=0.7871,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2219} INFO - iteration 146, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2392} INFO -  at 60.5s,\testimator xgb_limitdepth's best error=0.7347,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2219} INFO - iteration 144, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2392} INFO -  at 60.5s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:30] {2219} INFO - iteration 143, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2392} INFO -  at 60.7s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2219} INFO - iteration 223, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2392} INFO -  at 60.6s,\testimator xgb_limitdepth's best error=0.7347,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2219} INFO - iteration 145, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2392} INFO -  at 60.8s,\testimator xgb_limitdepth's best error=0.7628,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2219} INFO - iteration 144, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2392} INFO -  at 60.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2219} INFO - iteration 224, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2392} INFO -  at 60.8s,\testimator xgb_limitdepth's best error=0.7347,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2219} INFO - iteration 146, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2392} INFO -  at 60.9s,\testimator rf's best error=0.8299,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2219} INFO - iteration 147, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2392} INFO -  at 61.0s,\testimator xgb_limitdepth's best error=0.7628,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2219} INFO - iteration 145, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2392} INFO -  at 61.1s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2219} INFO - iteration 144, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2392} INFO -  at 61.1s,\testimator xgb_limitdepth's best error=0.7628,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2219} INFO - iteration 146, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2392} INFO -  at 61.2s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2219} INFO - iteration 225, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2392} INFO -  at 61.5s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:31] {2219} INFO - iteration 226, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:32] {2392} INFO -  at 61.6s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:32] {2219} INFO - iteration 227, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:32] {2392} INFO -  at 61.8s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:32] {2219} INFO - iteration 145, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:32] {2392} INFO -  at 62.3s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:32] {2219} INFO - iteration 148, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:32] {2392} INFO -  at 62.4s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:32] {2219} INFO - iteration 146, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2392} INFO -  at 62.7s,\testimator lgbm's best error=0.7516,\tbest estimator lgbm's best error=0.7516\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2219} INFO - iteration 147, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2392} INFO -  at 62.7s,\testimator rf's best error=0.7694,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2219} INFO - iteration 147, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2392} INFO -  at 63.1s,\testimator rf's best error=0.7694,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2219} INFO - iteration 148, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2392} INFO -  at 63.1s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2219} INFO - iteration 147, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2392} INFO -  at 63.3s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2219} INFO - iteration 148, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2392} INFO -  at 63.4s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:33] {2219} INFO - iteration 149, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:34] {2392} INFO -  at 64.2s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:34] {2219} INFO - iteration 148, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:34] {2392} INFO -  at 64.4s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:34] {2219} INFO - iteration 149, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:34] {2392} INFO -  at 64.4s,\testimator rf's best error=0.8299,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:34] {2219} INFO - iteration 149, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:34] {2392} INFO -  at 64.6s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:34] {2219} INFO - iteration 150, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 64.7s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 228, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 64.7s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 149, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 64.7s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 151, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 64.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 229, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 64.8s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 150, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 64.9s,\testimator xgboost's best error=0.7558,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 230, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 65.1s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 231, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 65.4s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 232, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 65.4s,\testimator rf's best error=0.7694,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 150, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 65.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 233, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 65.5s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 151, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 65.6s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2392} INFO -  at 65.6s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 234, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:35] {2219} INFO - iteration 152, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2392} INFO -  at 65.7s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2219} INFO - iteration 151, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2392} INFO -  at 65.9s,\testimator rf's best error=0.7626,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2219} INFO - iteration 152, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2392} INFO -  at 66.0s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2219} INFO - iteration 152, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2392} INFO -  at 66.0s,\testimator xgb_limitdepth's best error=0.7347,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2219} INFO - iteration 153, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2392} INFO -  at 66.1s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2219} INFO - iteration 235, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2392} INFO -  at 66.2s,\testimator xgb_limitdepth's best error=0.7628,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2219} INFO - iteration 153, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2392} INFO -  at 66.2s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2219} INFO - iteration 236, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2392} INFO -  at 66.2s,\testimator xgb_limitdepth's best error=0.7347,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2219} INFO - iteration 154, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2392} INFO -  at 66.3s,\testimator xgb_limitdepth's best error=0.7481,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2219} INFO - iteration 154, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2392} INFO -  at 66.5s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:36] {2219} INFO - iteration 237, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 66.6s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 238, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 66.7s,\testimator xgb_limitdepth's best error=0.7871,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 153, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 66.8s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 239, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 66.9s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 150, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 66.9s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 240, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 66.9s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 155, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 67.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 241, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 67.1s,\testimator lgbm's best error=0.7070,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 155, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 67.3s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 242, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 67.3s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 156, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 67.3s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 156, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 67.4s,\testimator xgb_limitdepth's best error=0.7347,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 157, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 67.4s,\testimator xgb_limitdepth's best error=0.7481,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 157, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 67.4s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 243, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 67.6s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 244, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 67.5s,\testimator xgb_limitdepth's best error=0.7347,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 158, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2392} INFO -  at 67.5s,\testimator xgb_limitdepth's best error=0.7481,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:37] {2219} INFO - iteration 158, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2392} INFO -  at 67.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2219} INFO - iteration 245, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2392} INFO -  at 67.8s,\testimator xgb_limitdepth's best error=0.7871,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2219} INFO - iteration 154, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2392} INFO -  at 67.8s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2219} INFO - iteration 159, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2392} INFO -  at 67.8s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2219} INFO - iteration 246, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2392} INFO -  at 68.0s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2219} INFO - iteration 160, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2392} INFO -  at 68.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2219} INFO - iteration 247, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2392} INFO -  at 68.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2219} INFO - iteration 248, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2392} INFO -  at 68.1s,\testimator xgb_limitdepth's best error=0.7481,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2219} INFO - iteration 161, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2392} INFO -  at 68.3s,\testimator lgbm's best error=0.6927,\tbest estimator lgbm's best error=0.6927\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2219} INFO - iteration 159, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2392} INFO -  at 68.4s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2219} INFO - iteration 151, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2392} INFO -  at 68.5s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:38] {2219} INFO - iteration 162, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:39] {2392} INFO -  at 68.8s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:39] {2219} INFO - iteration 155, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:39] {2392} INFO -  at 68.8s,\testimator xgb_limitdepth's best error=0.7475,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:39] {2219} INFO - iteration 163, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:39] {2392} INFO -  at 69.2s,\testimator xgb_limitdepth's best error=0.7475,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:39] {2219} INFO - iteration 164, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:39] {2392} INFO -  at 69.4s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:39] {2219} INFO - iteration 156, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2392} INFO -  at 69.7s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2219} INFO - iteration 165, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2392} INFO -  at 69.9s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2219} INFO - iteration 152, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2392} INFO -  at 69.9s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2219} INFO - iteration 166, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2392} INFO -  at 70.0s,\testimator lgbm's best error=0.6894,\tbest estimator lgbm's best error=0.6894\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2219} INFO - iteration 160, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2392} INFO -  at 70.2s,\testimator xgb_limitdepth's best error=0.7347,\tbest estimator lgbm's best error=0.6894\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2219} INFO - iteration 161, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2392} INFO -  at 70.3s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2219} INFO - iteration 167, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2392} INFO -  at 70.6s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2219} INFO - iteration 153, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2392} INFO -  at 70.6s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:40] {2219} INFO - iteration 162, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2392} INFO -  at 70.7s,\testimator xgb_limitdepth's best error=0.7871,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2219} INFO - iteration 157, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2392} INFO -  at 70.9s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2219} INFO - iteration 154, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2392} INFO -  at 70.9s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2219} INFO - iteration 168, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2392} INFO -  at 71.1s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2219} INFO - iteration 163, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2392} INFO -  at 71.2s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2219} INFO - iteration 169, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2392} INFO -  at 71.3s,\testimator xgb_limitdepth's best error=0.7347,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:41] {2219} INFO - iteration 164, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:42] {2392} INFO -  at 71.9s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:42] {2219} INFO - iteration 165, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:42] {2392} INFO -  at 72.1s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:42] {2219} INFO - iteration 166, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:42] {2392} INFO -  at 72.2s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:42] {2219} INFO - iteration 155, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2392} INFO -  at 72.9s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2219} INFO - iteration 156, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2392} INFO -  at 73.0s,\testimator rf's best error=0.7635,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2219} INFO - iteration 170, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2392} INFO -  at 73.2s,\testimator rf's best error=0.7683,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2219} INFO - iteration 249, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2392} INFO -  at 73.4s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2219} INFO - iteration 250, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2392} INFO -  at 73.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2219} INFO - iteration 251, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2392} INFO -  at 73.6s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:43] {2219} INFO - iteration 252, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2392} INFO -  at 73.7s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2219} INFO - iteration 253, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2392} INFO -  at 73.7s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2219} INFO - iteration 157, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2392} INFO -  at 73.8s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2219} INFO - iteration 158, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2392} INFO -  at 73.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2219} INFO - iteration 254, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2392} INFO -  at 74.0s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2219} INFO - iteration 255, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2392} INFO -  at 74.0s,\testimator xgb_limitdepth's best error=0.7871,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2219} INFO - iteration 158, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2392} INFO -  at 74.4s,\testimator rf's best error=0.8299,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:44] {2219} INFO - iteration 159, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2392} INFO -  at 75.9s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2219} INFO - iteration 256, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2392} INFO -  at 75.8s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2219} INFO - iteration 167, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2392} INFO -  at 76.3s,\testimator rf's best error=0.8237,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2219} INFO - iteration 160, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2392} INFO -  at 76.3s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2219} INFO - iteration 257, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2392} INFO -  at 76.4s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2219} INFO - iteration 258, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2392} INFO -  at 76.6s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:46] {2219} INFO - iteration 259, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2392} INFO -  at 76.7s,\testimator xgb_limitdepth's best error=0.7871,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2219} INFO - iteration 161, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2392} INFO -  at 76.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2219} INFO - iteration 260, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2392} INFO -  at 76.8s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2219} INFO - iteration 162, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2392} INFO -  at 77.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2219} INFO - iteration 261, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2392} INFO -  at 77.0s,\testimator extra_tree's best error=0.8154,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2219} INFO - iteration 163, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2392} INFO -  at 77.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2219} INFO - iteration 262, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2392} INFO -  at 77.3s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2219} INFO - iteration 263, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2392} INFO -  at 77.5s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2219} INFO - iteration 264, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2392} INFO -  at 77.6s,\testimator lgbm's best error=0.7070,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2219} INFO - iteration 171, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2392} INFO -  at 77.6s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:47] {2219} INFO - iteration 265, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 77.6s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 168, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 77.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 266, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 77.7s,\testimator xgb_limitdepth's best error=0.7347,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 169, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 77.8s,\testimator lgbm's best error=0.7070,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 172, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 77.9s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 267, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 77.9s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 173, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 78.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 268, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 78.0s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 170, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 78.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 269, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 78.1s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 171, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 78.2s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 174, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 78.3s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 270, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 78.5s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 271, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 78.5s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 175, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2392} INFO -  at 78.6s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:48] {2219} INFO - iteration 272, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2392} INFO -  at 78.7s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2219} INFO - iteration 273, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2392} INFO -  at 78.7s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2219} INFO - iteration 176, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2392} INFO -  at 78.8s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2219} INFO - iteration 274, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2392} INFO -  at 79.0s,\testimator xgb_limitdepth's best error=0.7475,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2219} INFO - iteration 177, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2392} INFO -  at 79.1s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2219} INFO - iteration 275, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2392} INFO -  at 79.3s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2219} INFO - iteration 276, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2392} INFO -  at 79.3s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2219} INFO - iteration 178, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2392} INFO -  at 79.4s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2219} INFO - iteration 277, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2392} INFO -  at 79.4s,\testimator xgboost's best error=0.7043,\tbest estimator xgboost's best error=0.7043\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2219} INFO - iteration 179, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2392} INFO -  at 79.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:49] {2219} INFO - iteration 278, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2392} INFO -  at 79.6s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2219} INFO - iteration 180, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2392} INFO -  at 79.7s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2219} INFO - iteration 279, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2392} INFO -  at 79.9s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2219} INFO - iteration 181, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2392} INFO -  at 80.0s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2219} INFO - iteration 280, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2392} INFO -  at 80.0s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2219} INFO - iteration 182, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2392} INFO -  at 80.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2219} INFO - iteration 281, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2392} INFO -  at 80.2s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2219} INFO - iteration 159, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2392} INFO -  at 80.5s,\testimator rf's best error=0.8092,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2219} INFO - iteration 160, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2392} INFO -  at 80.5s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2219} INFO - iteration 183, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2392} INFO -  at 80.6s,\testimator xgb_limitdepth's best error=0.7871,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:50] {2219} INFO - iteration 164, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2392} INFO -  at 80.8s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2219} INFO - iteration 172, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2392} INFO -  at 81.2s,\testimator rf's best error=0.7683,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2219} INFO - iteration 282, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2392} INFO -  at 81.4s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2219} INFO - iteration 161, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2392} INFO -  at 81.4s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2219} INFO - iteration 283, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2392} INFO -  at 81.6s,\testimator rf's best error=0.7626,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2219} INFO - iteration 173, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2392} INFO -  at 81.6s,\testimator rf's best error=0.7635,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:51] {2219} INFO - iteration 184, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 81.7s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 284, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 81.8s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 185, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 81.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 285, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 82.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 286, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 82.1s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 287, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 82.1s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 186, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 82.2s,\testimator xgb_limitdepth's best error=0.7871,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 165, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 82.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 288, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 82.3s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 166, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 82.3s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 187, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 82.5s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 289, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 82.5s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 188, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2392} INFO -  at 82.6s,\testimator xgboost's best error=0.7256,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:52] {2219} INFO - iteration 174, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2392} INFO -  at 82.7s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2219} INFO - iteration 290, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2392} INFO -  at 83.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2219} INFO - iteration 291, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2392} INFO -  at 83.0s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2219} INFO - iteration 167, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2392} INFO -  at 83.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2219} INFO - iteration 292, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2392} INFO -  at 83.3s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2219} INFO - iteration 293, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2392} INFO -  at 83.4s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2219} INFO - iteration 294, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2392} INFO -  at 83.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:53] {2219} INFO - iteration 295, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2392} INFO -  at 83.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2219} INFO - iteration 296, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2392} INFO -  at 83.9s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2219} INFO - iteration 297, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2392} INFO -  at 83.9s,\testimator xgboost's best error=0.7256,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2219} INFO - iteration 175, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2392} INFO -  at 84.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2219} INFO - iteration 298, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2392} INFO -  at 84.2s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2219} INFO - iteration 299, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2392} INFO -  at 84.4s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:54] {2219} INFO - iteration 300, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2392} INFO -  at 85.2s,\testimator rf's best error=0.7635,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2219} INFO - iteration 189, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2392} INFO -  at 85.2s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2219} INFO - iteration 176, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2392} INFO -  at 85.3s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2219} INFO - iteration 168, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2392} INFO -  at 85.3s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2219} INFO - iteration 190, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2392} INFO -  at 85.4s,\testimator xgb_limitdepth's best error=0.7299,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2219} INFO - iteration 177, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2392} INFO -  at 85.6s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:55] {2219} INFO - iteration 301, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2392} INFO -  at 85.7s,\testimator rf's best error=0.7626,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2219} INFO - iteration 178, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2392} INFO -  at 85.7s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2219} INFO - iteration 302, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2392} INFO -  at 85.9s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2219} INFO - iteration 191, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2392} INFO -  at 86.0s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2219} INFO - iteration 303, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2392} INFO -  at 86.0s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2219} INFO - iteration 179, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2392} INFO -  at 86.3s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2219} INFO - iteration 162, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2392} INFO -  at 86.4s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2219} INFO - iteration 304, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2392} INFO -  at 86.4s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:56] {2219} INFO - iteration 192, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2392} INFO -  at 86.7s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2219} INFO - iteration 163, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2392} INFO -  at 86.7s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2219} INFO - iteration 193, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2392} INFO -  at 86.9s,\testimator xgb_limitdepth's best error=0.7871,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2219} INFO - iteration 169, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2392} INFO -  at 87.1s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2219} INFO - iteration 194, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2392} INFO -  at 87.3s,\testimator extra_tree's best error=0.7691,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2219} INFO - iteration 195, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2392} INFO -  at 87.5s,\testimator rf's best error=0.7683,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:57] {2219} INFO - iteration 305, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 87.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 306, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 87.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 307, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 87.8s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 196, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 87.9s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 308, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 88.0s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 170, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 88.0s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 197, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 88.1s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 171, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 88.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 309, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 88.2s,\testimator extra_tree's best error=0.7691,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 198, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 88.2s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 310, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 88.4s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 311, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 88.3s,\testimator extra_tree's best error=0.7691,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 199, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 88.5s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 312, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2392} INFO -  at 88.5s,\testimator extra_tree's best error=0.7559,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:58] {2219} INFO - iteration 200, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 88.6s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 313, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 88.7s,\testimator extra_tree's best error=0.7559,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 201, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 88.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 314, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 88.9s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 180, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 88.9s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 315, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 88.9s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 164, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 89.1s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 316, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 89.1s,\testimator xgb_limitdepth's best error=0.7299,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 181, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 89.1s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 172, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 89.1s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 165, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 89.4s,\testimator xgb_limitdepth's best error=0.7606,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 317, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 89.4s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 166, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 89.5s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 318, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 89.5s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 173, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2392} INFO -  at 89.6s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:16:59] {2219} INFO - iteration 167, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2392} INFO -  at 89.8s,\testimator xgboost's best error=0.7642,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2219} INFO - iteration 168, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2392} INFO -  at 90.0s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2219} INFO - iteration 174, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2392} INFO -  at 90.0s,\testimator xgboost's best error=0.7256,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2219} INFO - iteration 182, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2392} INFO -  at 90.1s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2219} INFO - iteration 183, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2392} INFO -  at 90.3s,\testimator lgbm's best error=0.7070,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2219} INFO - iteration 202, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2392} INFO -  at 90.5s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2219} INFO - iteration 169, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2392} INFO -  at 90.5s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:00] {2219} INFO - iteration 203, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2392} INFO -  at 90.7s,\testimator extra_tree's best error=0.7559,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2219} INFO - iteration 204, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2392} INFO -  at 91.0s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2219} INFO - iteration 184, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2392} INFO -  at 91.0s,\testimator extra_tree's best error=0.7528,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2219} INFO - iteration 205, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2392} INFO -  at 91.2s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2219} INFO - iteration 175, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2392} INFO -  at 91.2s,\testimator extra_tree's best error=0.7527,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2219} INFO - iteration 206, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2392} INFO -  at 91.3s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2219} INFO - iteration 207, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2392} INFO -  at 91.6s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:01] {2219} INFO - iteration 319, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2392} INFO -  at 91.6s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2219} INFO - iteration 185, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2392} INFO -  at 91.7s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2219} INFO - iteration 208, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2392} INFO -  at 91.8s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2219} INFO - iteration 320, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2392} INFO -  at 92.1s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2219} INFO - iteration 209, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2392} INFO -  at 92.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2219} INFO - iteration 321, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2392} INFO -  at 92.2s,\testimator rf's best error=0.7626,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2219} INFO - iteration 186, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2392} INFO -  at 92.4s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:02] {2219} INFO - iteration 322, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2392} INFO -  at 92.6s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2219} INFO - iteration 187, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2392} INFO -  at 92.7s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2219} INFO - iteration 210, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2392} INFO -  at 92.8s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2219} INFO - iteration 323, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2392} INFO -  at 92.9s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2219} INFO - iteration 211, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2392} INFO -  at 93.0s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2219} INFO - iteration 324, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2392} INFO -  at 93.1s,\testimator xgb_limitdepth's best error=0.7299,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2219} INFO - iteration 188, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2392} INFO -  at 93.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2219} INFO - iteration 325, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2392} INFO -  at 93.6s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2219} INFO - iteration 326, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2392} INFO -  at 93.6s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:03] {2219} INFO - iteration 212, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2392} INFO -  at 93.7s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2219} INFO - iteration 170, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2392} INFO -  at 93.8s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2219} INFO - iteration 327, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2392} INFO -  at 94.1s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2219} INFO - iteration 328, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2392} INFO -  at 94.1s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2219} INFO - iteration 213, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2392} INFO -  at 94.4s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2219} INFO - iteration 329, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2392} INFO -  at 94.4s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2219} INFO - iteration 214, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2392} INFO -  at 94.5s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:04] {2219} INFO - iteration 330, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2392} INFO -  at 94.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2219} INFO - iteration 331, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2392} INFO -  at 94.8s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2219} INFO - iteration 171, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2392} INFO -  at 94.9s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2219} INFO - iteration 215, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2392} INFO -  at 95.2s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2219} INFO - iteration 216, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2392} INFO -  at 95.4s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2219} INFO - iteration 189, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2392} INFO -  at 95.5s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:05] {2219} INFO - iteration 217, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2392} INFO -  at 96.3s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2219} INFO - iteration 190, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2392} INFO -  at 96.4s,\testimator rf's best error=0.8186,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2219} INFO - iteration 176, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2392} INFO -  at 96.3s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2219} INFO - iteration 218, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2392} INFO -  at 96.5s,\testimator lgbm's best error=0.7511,\tbest estimator lgbm's best error=0.7511\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2219} INFO - iteration 172, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2392} INFO -  at 96.5s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2219} INFO - iteration 219, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2392} INFO -  at 96.5s,\testimator xgb_limitdepth's best error=0.7299,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:06] {2219} INFO - iteration 191, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 96.7s,\testimator xgb_limitdepth's best error=0.7871,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 177, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 96.7s,\testimator xgboost's best error=0.7493,\tbest estimator xgboost's best error=0.7493\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 173, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 96.9s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 332, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 96.9s,\testimator xgboost's best error=0.7493,\tbest estimator xgboost's best error=0.7493\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 174, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 96.9s,\testimator xgboost's best error=0.7024,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 220, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 97.0s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 333, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 97.0s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 192, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 97.2s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 334, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 97.2s,\testimator xgboost's best error=0.7493,\tbest estimator xgboost's best error=0.7493\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 175, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 97.3s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 335, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 97.3s,\testimator rf's best error=0.7626,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 193, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 97.4s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 221, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 97.5s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 336, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2392} INFO -  at 97.6s,\testimator xgboost's best error=0.7493,\tbest estimator xgboost's best error=0.7493\n",
            "[flaml.automl.logger: 06-30 11:17:07] {2219} INFO - iteration 176, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 97.7s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 337, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 97.6s,\testimator rf's best error=0.7626,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 194, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 97.8s,\testimator xgboost's best error=0.7466,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 338, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 97.8s,\testimator lgbm's best error=0.7070,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 222, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 97.9s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 339, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 97.9s,\testimator xgb_limitdepth's best error=0.7299,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 195, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 98.1s,\testimator extra_tree's best error=0.7339,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 340, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 98.1s,\testimator xgb_limitdepth's best error=0.7523,\tbest estimator xgboost's best error=0.7493\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 177, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 98.2s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 178, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 98.2s,\testimator extra_tree's best error=0.7296,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 223, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 98.3s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 179, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 98.5s,\testimator extra_tree's best error=0.7284,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 224, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2392} INFO -  at 98.6s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:08] {2219} INFO - iteration 196, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2392} INFO -  at 98.7s,\testimator xgboost's best error=0.7493,\tbest estimator xgboost's best error=0.7493\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2219} INFO - iteration 178, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2392} INFO -  at 98.8s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2219} INFO - iteration 180, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2392} INFO -  at 99.0s,\testimator xgboost's best error=0.7493,\tbest estimator xgboost's best error=0.7493\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2219} INFO - iteration 179, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2392} INFO -  at 99.4s,\testimator lgbm's best error=0.6877,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2219} INFO - iteration 197, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2392} INFO -  at 99.5s,\testimator xgboost's best error=0.7850,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2219} INFO - iteration 181, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2392} INFO -  at 99.5s,\testimator xgboost's best error=0.7493,\tbest estimator xgboost's best error=0.7493\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2219} INFO - iteration 180, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2392} INFO -  at 99.5s,\testimator extra_tree's best error=0.7647,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:09] {2219} INFO - iteration 198, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 99.7s,\testimator xgb_limitdepth's best error=0.7299,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 199, current learner xgb_limitdepth\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 99.8s,\testimator xgboost's best error=0.7493,\tbest estimator xgboost's best error=0.7493\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2628} INFO - retrain xgboost for 0.2s\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2631} INFO - retrained model: XGBRegressor(base_score=None, booster=None, callbacks=[],\n",
            "             colsample_bylevel=0.5537952559179024, colsample_bynode=None,\n",
            "             colsample_bytree=0.7299700850935068, device=None,\n",
            "             early_stopping_rounds=None, enable_categorical=False,\n",
            "             eval_metric=None, feature_types=None, gamma=None,\n",
            "             grow_policy='lossguide', importance_type=None,\n",
            "             interaction_constraints=None, learning_rate=0.091712116308392,\n",
            "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "             max_delta_step=None, max_depth=0, max_leaves=6,\n",
            "             min_child_weight=42.230808738816165, missing=nan,\n",
            "             monotone_constraints=None, multi_strategy=None, n_estimators=61,\n",
            "             n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1931} INFO - fit succeeded\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1932} INFO - Time taken to find the best model: 96.70418000221252\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 99.9s,\testimator lgbm's best error=0.7952,\tbest estimator xgboost's best error=0.7850\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 99.9s,\testimator xgb_limitdepth's best error=0.7299,\tbest estimator lgbm's best error=0.6877\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 99.9s,\testimator lgbm's best error=0.7070,\tbest estimator xgboost's best error=0.7024\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 100.0s,\testimator lgbm's best error=0.7454,\tbest estimator extra_tree's best error=0.7339\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2628} INFO - retrain xgboost for 0.1s\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2628} INFO - retrain extra_tree for 0.0s\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2631} INFO - retrained model: XGBRegressor(base_score=None, booster=None, callbacks=[],\n",
            "             colsample_bylevel=0.8402755288273344, colsample_bynode=None,\n",
            "             colsample_bytree=0.8843664717482072, device=None,\n",
            "             early_stopping_rounds=None, enable_categorical=False,\n",
            "             eval_metric=None, feature_types=None, gamma=None,\n",
            "             grow_policy='lossguide', importance_type=None,\n",
            "             interaction_constraints=None, learning_rate=0.11216016897269067,\n",
            "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "             max_delta_step=None, max_depth=0, max_leaves=4,\n",
            "             min_child_weight=0.2837524261938666, missing=nan,\n",
            "             monotone_constraints=None, multi_strategy=None, n_estimators=41,\n",
            "             n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1931} INFO - fit succeeded\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1932} INFO - Time taken to find the best model: 79.62944507598877\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2631} INFO - retrained model: ExtraTreesRegressor(max_leaf_nodes=10, n_estimators=4, n_jobs=-1,\n",
            "                    random_state=12032022)\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1931} INFO - fit succeeded\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1932} INFO - Time taken to find the best model: 14.71803879737854\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2628} INFO - retrain lgbm for 0.1s\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2631} INFO - retrained model: LGBMRegressor(colsample_bytree=0.9073584164636029,\n",
            "              learning_rate=0.1989674511160669, max_bin=511,\n",
            "              min_child_samples=5, n_estimators=1, n_jobs=-1, num_leaves=6,\n",
            "              reg_alpha=0.0056650156730923594, reg_lambda=34.74483405786768,\n",
            "              verbose=-1)\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1931} INFO - fit succeeded\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1932} INFO - Time taken to find the best model: 70.57748413085938\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2628} INFO - retrain xgboost for 0.2s\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2631} INFO - retrained model: XGBRegressor(base_score=None, booster=None, callbacks=[],\n",
            "             colsample_bylevel=0.8275122242334021, colsample_bynode=None,\n",
            "             colsample_bytree=0.7322952724525803, device=None,\n",
            "             early_stopping_rounds=None, enable_categorical=False,\n",
            "             eval_metric=None, feature_types=None, gamma=None,\n",
            "             grow_policy='lossguide', importance_type=None,\n",
            "             interaction_constraints=None, learning_rate=0.0853149816502942,\n",
            "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "             max_delta_step=None, max_depth=0, max_leaves=4,\n",
            "             min_child_weight=44.1289259287229, missing=nan,\n",
            "             monotone_constraints=None, multi_strategy=None, n_estimators=198,\n",
            "             n_jobs=-1, num_parallel_tree=None, random_state=None, ...)\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1931} INFO - fit succeeded\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1932} INFO - Time taken to find the best model: 22.832711935043335\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1680} INFO - task = classification\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1680} INFO - task = classification\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1789} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1789} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'lrl1']\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'lrl1']\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1680} INFO - task = classification\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1789} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1680} INFO - task = classification\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'lrl1']\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1789} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'lrl1']\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2345} INFO - Estimated sufficient time budget=1030s. Estimated necessary time budget=24s.\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 0.1s,\testimator lgbm's best error=1.5921,\tbest estimator lgbm's best error=1.5921\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2345} INFO - Estimated sufficient time budget=966s. Estimated necessary time budget=22s.\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2345} INFO - Estimated sufficient time budget=1042s. Estimated necessary time budget=24s.\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 0.1s,\testimator lgbm's best error=1.5902,\tbest estimator lgbm's best error=1.5902\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 0.1s,\testimator lgbm's best error=1.5845,\tbest estimator lgbm's best error=1.5845\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2345} INFO - Estimated sufficient time budget=998s. Estimated necessary time budget=23s.\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 0.1s,\testimator lgbm's best error=1.5867,\tbest estimator lgbm's best error=1.5867\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 0.2s,\testimator lgbm's best error=1.5845,\tbest estimator lgbm's best error=1.5845\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 0.2s,\testimator lgbm's best error=1.5867,\tbest estimator lgbm's best error=1.5867\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 0.2s,\testimator lgbm's best error=1.5921,\tbest estimator lgbm's best error=1.5921\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 0.2s,\testimator lgbm's best error=1.5902,\tbest estimator lgbm's best error=1.5902\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2392} INFO -  at 0.3s,\testimator lgbm's best error=1.3679,\tbest estimator lgbm's best error=1.3679\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1680} INFO - task = classification\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1691} INFO - Evaluation method: cv\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1789} INFO - Minimizing error metric: 1-r2\n",
            "[flaml.automl.logger: 06-30 11:17:10] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'lrl1']\n",
            "[flaml.automl.logger: 06-30 11:17:10] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.4s,\testimator lgbm's best error=1.4039,\tbest estimator lgbm's best error=1.4039\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.4s,\testimator lgbm's best error=1.3871,\tbest estimator lgbm's best error=1.3871\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.4s,\testimator lgbm's best error=1.3687,\tbest estimator lgbm's best error=1.3687\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.5s,\testimator xgboost's best error=1.5845,\tbest estimator lgbm's best error=1.3679\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2345} INFO - Estimated sufficient time budget=1876s. Estimated necessary time budget=43s.\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.2s,\testimator lgbm's best error=1.6002,\tbest estimator lgbm's best error=1.6002\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.6s,\testimator xgboost's best error=1.5867,\tbest estimator lgbm's best error=1.4039\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.6s,\testimator xgboost's best error=1.5902,\tbest estimator lgbm's best error=1.3871\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.6s,\testimator xgboost's best error=1.5921,\tbest estimator lgbm's best error=1.3687\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.7s,\testimator lgbm's best error=1.3516,\tbest estimator lgbm's best error=1.3516\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.4s,\testimator lgbm's best error=1.6002,\tbest estimator lgbm's best error=1.6002\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.7s,\testimator lgbm's best error=1.3590,\tbest estimator lgbm's best error=1.3590\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.7s,\testimator lgbm's best error=1.3758,\tbest estimator lgbm's best error=1.3758\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.8s,\testimator lgbm's best error=1.3514,\tbest estimator lgbm's best error=1.3514\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.8s,\testimator lgbm's best error=1.3516,\tbest estimator lgbm's best error=1.3516\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.8s,\testimator lgbm's best error=1.3758,\tbest estimator lgbm's best error=1.3758\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.5s,\testimator lgbm's best error=1.3893,\tbest estimator lgbm's best error=1.3893\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 3, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.9s,\testimator lgbm's best error=1.3590,\tbest estimator lgbm's best error=1.3590\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.9s,\testimator lgbm's best error=1.3514,\tbest estimator lgbm's best error=1.3514\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.7s,\testimator xgboost's best error=1.6002,\tbest estimator lgbm's best error=1.3893\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.0s,\testimator lgbm's best error=1.3516,\tbest estimator lgbm's best error=1.3516\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.0s,\testimator lgbm's best error=1.3590,\tbest estimator lgbm's best error=1.3590\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.0s,\testimator lgbm's best error=1.3725,\tbest estimator lgbm's best error=1.3725\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.1s,\testimator lgbm's best error=1.3514,\tbest estimator lgbm's best error=1.3514\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.2s,\testimator lgbm's best error=1.3305,\tbest estimator lgbm's best error=1.3305\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.2s,\testimator lgbm's best error=1.3379,\tbest estimator lgbm's best error=1.3379\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 0.9s,\testimator lgbm's best error=1.3382,\tbest estimator lgbm's best error=1.3382\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.2s,\testimator lgbm's best error=1.3725,\tbest estimator lgbm's best error=1.3725\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.3s,\testimator lgbm's best error=1.3443,\tbest estimator lgbm's best error=1.3443\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.0s,\testimator lgbm's best error=1.3382,\tbest estimator lgbm's best error=1.3382\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.3s,\testimator lgbm's best error=1.3305,\tbest estimator lgbm's best error=1.3305\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.3s,\testimator lgbm's best error=1.3379,\tbest estimator lgbm's best error=1.3379\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2392} INFO -  at 1.3s,\testimator lgbm's best error=1.3725,\tbest estimator lgbm's best error=1.3725\n",
            "[flaml.automl.logger: 06-30 11:17:11] {2219} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.4s,\testimator lgbm's best error=1.3443,\tbest estimator lgbm's best error=1.3443\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.4s,\testimator xgboost's best error=1.5845,\tbest estimator lgbm's best error=1.3305\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.4s,\testimator xgboost's best error=1.5921,\tbest estimator lgbm's best error=1.3379\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.1s,\testimator lgbm's best error=1.3382,\tbest estimator lgbm's best error=1.3382\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.4s,\testimator xgboost's best error=1.5902,\tbest estimator lgbm's best error=1.3725\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.5s,\testimator xgboost's best error=1.5867,\tbest estimator lgbm's best error=1.3443\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.5s,\testimator xgboost's best error=1.3868,\tbest estimator lgbm's best error=1.3305\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 11, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.5s,\testimator xgboost's best error=1.3779,\tbest estimator lgbm's best error=1.3379\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 11, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.2s,\testimator lgbm's best error=1.3285,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.6s,\testimator xgboost's best error=1.4120,\tbest estimator lgbm's best error=1.3725\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 11, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.6s,\testimator xgboost's best error=1.4049,\tbest estimator lgbm's best error=1.3443\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 11, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.3s,\testimator lgbm's best error=1.3285,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 9, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.7s,\testimator extra_tree's best error=1.5796,\tbest estimator lgbm's best error=1.3305\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 12, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.7s,\testimator extra_tree's best error=1.5684,\tbest estimator lgbm's best error=1.3379\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 12, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.7s,\testimator extra_tree's best error=1.5519,\tbest estimator lgbm's best error=1.3725\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 12, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.4s,\testimator xgboost's best error=1.6002,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 10, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.8s,\testimator extra_tree's best error=1.5802,\tbest estimator lgbm's best error=1.3443\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 12, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.8s,\testimator extra_tree's best error=1.4275,\tbest estimator lgbm's best error=1.3379\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 13, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.8s,\testimator extra_tree's best error=1.4307,\tbest estimator lgbm's best error=1.3305\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 13, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.5s,\testimator xgboost's best error=1.3925,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 11, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.8s,\testimator extra_tree's best error=1.4400,\tbest estimator lgbm's best error=1.3725\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 13, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.9s,\testimator xgboost's best error=1.3403,\tbest estimator lgbm's best error=1.3305\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.9s,\testimator xgboost's best error=1.3363,\tbest estimator xgboost's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 14, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 14, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.9s,\testimator extra_tree's best error=1.4006,\tbest estimator lgbm's best error=1.3443\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 13, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.6s,\testimator xgboost's best error=1.3544,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 12, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.9s,\testimator xgboost's best error=1.3585,\tbest estimator xgboost's best error=1.3585\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 14, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.0s,\testimator xgboost's best error=1.3514,\tbest estimator lgbm's best error=1.3443\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 14, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.0s,\testimator xgboost's best error=1.3294,\tbest estimator xgboost's best error=1.3294\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 15, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.0s,\testimator xgboost's best error=1.3352,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 15, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.7s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 13, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.0s,\testimator extra_tree's best error=1.4400,\tbest estimator xgboost's best error=1.3585\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 15, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.1s,\testimator xgboost's best error=1.3352,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 16, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.1s,\testimator xgboost's best error=1.3294,\tbest estimator xgboost's best error=1.3294\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 16, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 1.8s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 14, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.1s,\testimator extra_tree's best error=1.4006,\tbest estimator lgbm's best error=1.3443\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 15, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.1s,\testimator xgboost's best error=1.3585,\tbest estimator xgboost's best error=1.3585\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 16, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.2s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 16, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.2s,\testimator extra_tree's best error=1.4307,\tbest estimator xgboost's best error=1.3294\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 17, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.2s,\testimator extra_tree's best error=1.4275,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 17, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2392} INFO -  at 2.3s,\testimator extra_tree's best error=1.4400,\tbest estimator xgboost's best error=1.3585\n",
            "[flaml.automl.logger: 06-30 11:17:12] {2219} INFO - iteration 17, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.4s,\testimator extra_tree's best error=1.4006,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 17, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.1s,\testimator lgbm's best error=1.3285,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 15, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.4s,\testimator xgboost's best error=1.3471,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 18, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.5s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 18, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.5s,\testimator lgbm's best error=1.3363,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 18, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.5s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3294\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 18, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.2s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 16, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.6s,\testimator rf's best error=1.4239,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 19, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.3s,\testimator lgbm's best error=1.3285,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 17, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.6s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 19, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.7s,\testimator xgboost's best error=1.3352,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 19, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.7s,\testimator xgboost's best error=1.3294,\tbest estimator xgboost's best error=1.3294\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 19, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.7s,\testimator xgboost's best error=1.3471,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 20, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.4s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 18, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.8s,\testimator rf's best error=1.4607,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 20, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.8s,\testimator rf's best error=1.4189,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 20, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.8s,\testimator rf's best error=1.4480,\tbest estimator xgboost's best error=1.3294\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 20, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.5s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 19, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.9s,\testimator rf's best error=1.3763,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 21, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.9s,\testimator rf's best error=1.3627,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 21, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 3.0s,\testimator rf's best error=1.3865,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 21, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 3.0s,\testimator rf's best error=1.3733,\tbest estimator xgboost's best error=1.3294\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 21, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 3.0s,\testimator xgboost's best error=1.3471,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 22, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.7s,\testimator extra_tree's best error=1.5421,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 20, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 3.0s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 22, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 3.1s,\testimator xgboost's best error=1.3352,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 22, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 3.1s,\testimator xgboost's best error=1.3224,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 22, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 2.8s,\testimator extra_tree's best error=1.4506,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 21, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 3.1s,\testimator rf's best error=1.3763,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 23, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 3.1s,\testimator rf's best error=1.3627,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 23, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 3.2s,\testimator lgbm's best error=1.3363,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 23, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 3.2s,\testimator rf's best error=1.3733,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 23, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2392} INFO -  at 3.4s,\testimator rf's best error=1.3865,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:13] {2219} INFO - iteration 24, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.4s,\testimator rf's best error=1.3763,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 24, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.4s,\testimator rf's best error=1.3627,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 24, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.5s,\testimator rf's best error=1.3733,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 24, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.5s,\testimator extra_tree's best error=1.4275,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.5s,\testimator extra_tree's best error=1.3990,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.5s,\testimator extra_tree's best error=1.3752,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.6s,\testimator extra_tree's best error=1.4307,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.7s,\testimator extra_tree's best error=1.3763,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 26, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.7s,\testimator extra_tree's best error=1.3790,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 26, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.7s,\testimator extra_tree's best error=1.3638,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 26, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.8s,\testimator extra_tree's best error=1.3787,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 26, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.8s,\testimator xgboost's best error=1.3471,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 27, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.9s,\testimator rf's best error=1.3757,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 27, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.9s,\testimator rf's best error=1.3427,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 27, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.9s,\testimator rf's best error=1.3489,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 27, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.6s,\testimator lgbm's best error=1.3285,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 22, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.9s,\testimator xgboost's best error=1.3471,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 28, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 4.0s,\testimator xgboost's best error=1.3352,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 28, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 4.0s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 28, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 4.0s,\testimator xgboost's best error=1.3224,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 28, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.7s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 23, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 4.1s,\testimator extra_tree's best error=1.3790,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 29, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 4.2s,\testimator extra_tree's best error=1.3638,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 29, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 3.9s,\testimator extra_tree's best error=1.4506,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 24, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 4.2s,\testimator extra_tree's best error=1.3763,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 29, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 4.2s,\testimator extra_tree's best error=1.3701,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 29, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 4.3s,\testimator extra_tree's best error=1.3790,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 30, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 4.3s,\testimator extra_tree's best error=1.3763,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 4.3s,\testimator xgboost's best error=1.3224,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2392} INFO -  at 4.0s,\testimator rf's best error=1.4517,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:14] {2219} INFO - iteration 25, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 4.4s,\testimator extra_tree's best error=1.3638,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 30, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 4.4s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 4.5s,\testimator rf's best error=1.3607,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 31, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 4.2s,\testimator extra_tree's best error=1.4506,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 26, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 4.7s,\testimator rf's best error=1.3368,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 4.4s,\testimator rf's best error=1.3759,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 27, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 4.8s,\testimator rf's best error=1.3477,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 32, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 4.6s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 28, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 5.0s,\testimator rf's best error=1.3477,\tbest estimator xgboost's best error=1.3471\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 33, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 4.7s,\testimator extra_tree's best error=1.3839,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 29, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 5.1s,\testimator lgbm's best error=1.3422,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 32, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 5.1s,\testimator xgboost's best error=1.3390,\tbest estimator xgboost's best error=1.3390\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 34, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 5.3s,\testimator rf's best error=1.3368,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 33, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2392} INFO -  at 4.9s,\testimator extra_tree's best error=1.3839,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:15] {2219} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 5.4s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 34, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 5.6s,\testimator rf's best error=1.3380,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 35, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 5.3s,\testimator lgbm's best error=1.3285,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 5.6s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 32, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 5.7s,\testimator xgboost's best error=1.3390,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 36, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 5.5s,\testimator lgbm's best error=1.3285,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 32, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 5.9s,\testimator rf's best error=1.3297,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 35, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 5.9s,\testimator extra_tree's best error=1.3790,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 37, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 5.9s,\testimator rf's best error=1.3435,\tbest estimator xgboost's best error=1.3224\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 33, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 6.0s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 36, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 5.7s,\testimator rf's best error=1.3759,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 33, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 6.0s,\testimator xgboost's best error=1.3390,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 38, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 6.0s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 34, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 6.1s,\testimator extra_tree's best error=1.3638,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 37, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 5.8s,\testimator extra_tree's best error=1.3759,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 34, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 6.1s,\testimator xgboost's best error=1.3390,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 39, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 6.2s,\testimator rf's best error=1.3332,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 35, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 6.2s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 38, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 6.0s,\testimator lgbm's best error=1.3285,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 35, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2392} INFO -  at 6.3s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:16] {2219} INFO - iteration 36, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.4s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 39, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.4s,\testimator rf's best error=1.3380,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 40, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.5s,\testimator extra_tree's best error=1.3701,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 37, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.5s,\testimator xgboost's best error=1.3390,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 41, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.3s,\testimator rf's best error=1.3759,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 36, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.6s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 38, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.6s,\testimator rf's best error=1.3297,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 40, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.7s,\testimator xgboost's best error=1.3390,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 42, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.8s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 41, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.8s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 39, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.5s,\testimator extra_tree's best error=1.3759,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 37, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.9s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 42, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.7s,\testimator rf's best error=1.3549,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 38, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 7.0s,\testimator lgbm's best error=1.3596,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 43, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 6.8s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 39, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 7.1s,\testimator rf's best error=1.3332,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 40, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 7.1s,\testimator lgbm's best error=1.3422,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 43, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 7.1s,\testimator xgboost's best error=1.3390,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 44, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 7.2s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 44, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 7.3s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 41, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 7.3s,\testimator lgbm's best error=1.3363,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2392} INFO -  at 7.3s,\testimator extra_tree's best error=1.3790,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:17] {2219} INFO - iteration 45, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.4s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 42, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.1s,\testimator rf's best error=1.3517,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 40, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.4s,\testimator xgboost's best error=1.3390,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 46, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.5s,\testimator extra_tree's best error=1.3638,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 45, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.2s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 41, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.6s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 46, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.6s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 43, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.3s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 42, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.8s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 44, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.9s,\testimator rf's best error=1.3380,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 47, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 8.0s,\testimator xgboost's best error=1.3390,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 48, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 8.0s,\testimator lgbm's best error=1.3363,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 32, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.7s,\testimator lgbm's best error=1.3285,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 43, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 8.0s,\testimator rf's best error=1.3297,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 47, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 8.1s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 48, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 8.1s,\testimator extra_tree's best error=1.3701,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 45, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 8.1s,\testimator lgbm's best error=1.3596,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 7.8s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 49, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 44, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 8.1s,\testimator xgboost's best error=1.3352,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 33, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 8.3s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 46, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2392} INFO -  at 8.0s,\testimator extra_tree's best error=1.3759,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:18] {2219} INFO - iteration 45, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.4s,\testimator extra_tree's best error=1.3763,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 34, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.2s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 46, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.5s,\testimator rf's best error=1.3380,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 50, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.5s,\testimator rf's best error=1.3297,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 49, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.6s,\testimator rf's best error=1.3284,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 47, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.6s,\testimator rf's best error=1.3682,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 35, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.7s,\testimator xgboost's best error=1.3232,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 50, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.7s,\testimator extra_tree's best error=1.3704,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 51, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.7s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 48, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.4s,\testimator rf's best error=1.3517,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 47, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.7s,\testimator xgboost's best error=1.3352,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 36, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.8s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 49, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.5s,\testimator xgboost's best error=1.3296,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 48, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.9s,\testimator extra_tree's best error=1.3638,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 51, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.9s,\testimator extra_tree's best error=1.3763,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 37, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 9.0s,\testimator rf's best error=1.3380,\tbest estimator rf's best error=1.3380\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 52, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 9.0s,\testimator xgboost's best error=1.3352,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 38, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 9.1s,\testimator rf's best error=1.3284,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 50, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 9.1s,\testimator xgboost's best error=1.3352,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 39, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 9.2s,\testimator rf's best error=1.3297,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 52, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 9.2s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 51, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 8.9s,\testimator lgbm's best error=1.3285,\tbest estimator lgbm's best error=1.3285\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 49, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2392} INFO -  at 9.3s,\testimator xgboost's best error=1.3352,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:19] {2219} INFO - iteration 40, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2392} INFO -  at 9.0s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2219} INFO - iteration 50, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2392} INFO -  at 9.5s,\testimator rf's best error=1.3284,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2219} INFO - iteration 52, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2392} INFO -  at 9.6s,\testimator xgboost's best error=1.3352,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2219} INFO - iteration 41, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2392} INFO -  at 9.4s,\testimator extra_tree's best error=1.3759,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2219} INFO - iteration 51, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2392} INFO -  at 9.8s,\testimator rf's best error=1.3284,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2392} INFO -  at 9.8s,\testimator rf's best error=1.3363,\tbest estimator rf's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2219} INFO - iteration 53, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2219} INFO - iteration 53, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2392} INFO -  at 9.9s,\testimator extra_tree's best error=1.3763,\tbest estimator xgboost's best error=1.3352\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2219} INFO - iteration 42, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2392} INFO -  at 10.0s,\testimator rf's best error=1.3254,\tbest estimator xgboost's best error=1.3232\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2219} INFO - iteration 53, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2392} INFO -  at 9.9s,\testimator rf's best error=1.3517,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2219} INFO - iteration 52, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2392} INFO -  at 10.3s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2219} INFO - iteration 54, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2392} INFO -  at 10.3s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:20] {2219} INFO - iteration 43, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 10.1s,\testimator rf's best error=1.3517,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 53, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 10.4s,\testimator rf's best error=1.3211,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 54, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 10.5s,\testimator extra_tree's best error=1.3763,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 44, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 10.3s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 54, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 10.7s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 55, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 10.7s,\testimator extra_tree's best error=1.3763,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 45, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 10.5s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 55, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 10.8s,\testimator extra_tree's best error=1.3396,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 46, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 10.9s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 56, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 10.6s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 56, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 11.0s,\testimator rf's best error=1.3682,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 47, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 11.1s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 57, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 10.9s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 57, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 11.2s,\testimator extra_tree's best error=1.3396,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 48, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2392} INFO -  at 11.0s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:21] {2219} INFO - iteration 58, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 11.4s,\testimator rf's best error=1.3284,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 58, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 11.2s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 59, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 11.6s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 59, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 11.6s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 49, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 11.3s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 60, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 11.7s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 60, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 11.5s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 61, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 11.8s,\testimator extra_tree's best error=1.3396,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 50, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 11.9s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 61, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 12.0s,\testimator extra_tree's best error=1.3396,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 51, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 12.1s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 62, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 12.2s,\testimator rf's best error=1.3682,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 52, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 11.9s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 62, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 12.3s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 63, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2392} INFO -  at 12.0s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:22] {2219} INFO - iteration 63, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 12.4s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 64, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 12.4s,\testimator rf's best error=1.3606,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 53, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 12.1s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 64, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 12.6s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 65, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 12.8s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 66, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 12.4s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 65, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 12.8s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 54, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 12.5s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 66, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 12.9s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 67, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 12.6s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 67, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 13.1s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 68, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 12.9s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 68, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 13.2s,\testimator lgbm's best error=1.3305,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 69, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2392} INFO -  at 13.3s,\testimator lgbm's best error=1.3596,\tbest estimator rf's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:23] {2219} INFO - iteration 54, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.1s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 69, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.4s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 70, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.4s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 55, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.1s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 70, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.5s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 71, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.5s,\testimator lgbm's best error=1.3422,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 55, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.2s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 71, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.6s,\testimator extra_tree's best error=1.3396,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 56, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.7s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 72, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.4s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 72, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.9s,\testimator lgbm's best error=1.3596,\tbest estimator rf's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 55, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.9s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 57, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.9s,\testimator extra_tree's best error=1.3701,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 73, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.6s,\testimator extra_tree's best error=1.3759,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 73, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 14.0s,\testimator rf's best error=1.3211,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 14.0s,\testimator xgboost's best error=1.3380,\tbest estimator rf's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 56, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 56, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 14.1s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 74, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.8s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 74, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 14.2s,\testimator rf's best error=1.3606,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 58, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 13.9s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 75, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2392} INFO -  at 14.3s,\testimator extra_tree's best error=1.3701,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:24] {2219} INFO - iteration 75, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2392} INFO -  at 14.4s,\testimator extra_tree's best error=1.3396,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2219} INFO - iteration 59, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2392} INFO -  at 14.6s,\testimator rf's best error=1.3363,\tbest estimator rf's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2219} INFO - iteration 57, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2392} INFO -  at 14.6s,\testimator rf's best error=1.3211,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2219} INFO - iteration 57, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2392} INFO -  at 14.3s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2219} INFO - iteration 76, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2392} INFO -  at 14.9s,\testimator rf's best error=1.3211,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2219} INFO - iteration 58, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2392} INFO -  at 14.7s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2219} INFO - iteration 77, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2392} INFO -  at 15.0s,\testimator lgbm's best error=1.3208,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2219} INFO - iteration 76, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2392} INFO -  at 15.1s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2219} INFO - iteration 60, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2392} INFO -  at 14.8s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2219} INFO - iteration 78, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2392} INFO -  at 15.2s,\testimator rf's best error=1.3211,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2219} INFO - iteration 59, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2392} INFO -  at 15.0s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:25] {2219} INFO - iteration 79, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 15.5s,\testimator extra_tree's best error=1.3396,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 61, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 15.5s,\testimator lgbm's best error=1.3208,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 77, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 15.8s,\testimator rf's best error=1.3211,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 60, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 15.5s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 80, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 15.8s,\testimator xgboost's best error=1.3192,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 78, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 15.9s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 62, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 15.9s,\testimator xgboost's best error=1.3232,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 61, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 15.6s,\testimator extra_tree's best error=1.3759,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 81, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 15.8s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 82, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 16.2s,\testimator rf's best error=1.3284,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 79, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 16.2s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 63, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2392} INFO -  at 16.3s,\testimator rf's best error=1.3363,\tbest estimator rf's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:26] {2219} INFO - iteration 58, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 16.1s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 83, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 16.5s,\testimator rf's best error=1.3211,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 62, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 16.5s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 64, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 16.6s,\testimator lgbm's best error=1.3208,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 80, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 16.3s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 84, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 16.8s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 65, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 16.9s,\testimator extra_tree's best error=1.3701,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 81, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 16.9s,\testimator xgboost's best error=1.3232,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 63, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 16.6s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 85, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 17.1s,\testimator xgboost's best error=1.3232,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 64, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 17.1s,\testimator rf's best error=1.3363,\tbest estimator rf's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 59, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 16.8s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 86, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 17.2s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 66, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 17.2s,\testimator rf's best error=1.3284,\tbest estimator xgboost's best error=1.3192\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 82, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 16.9s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 87, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2392} INFO -  at 17.4s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:27] {2219} INFO - iteration 83, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.0s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 88, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.4s,\testimator lgbm's best error=1.3434,\tbest estimator rf's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 60, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.4s,\testimator rf's best error=1.3211,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 65, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.4s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 67, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.5s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 84, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.5s,\testimator xgboost's best error=1.3232,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 66, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.5s,\testimator xgboost's best error=1.3380,\tbest estimator rf's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 61, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.6s,\testimator xgboost's best error=1.3232,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 67, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.3s,\testimator rf's best error=1.3517,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 89, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.7s,\testimator rf's best error=1.3606,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 68, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.7s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 85, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.4s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 90, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.7s,\testimator xgboost's best error=1.3232,\tbest estimator rf's best error=1.3211\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 68, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.8s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 86, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.9s,\testimator lgbm's best error=1.3434,\tbest estimator rf's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 62, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.6s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 91, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 18.0s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 87, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 18.1s,\testimator xgboost's best error=1.3380,\tbest estimator rf's best error=1.3363\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 63, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 18.1s,\testimator rf's best error=1.3200,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 69, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 18.2s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 88, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 18.2s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 64, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 17.9s,\testimator extra_tree's best error=1.3414,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 92, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 18.2s,\testimator xgboost's best error=1.3232,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 70, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 18.3s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 65, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2392} INFO -  at 18.0s,\testimator extra_tree's best error=1.3414,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:28] {2219} INFO - iteration 93, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.4s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 69, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.4s,\testimator rf's best error=1.3284,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 89, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.4s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 66, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.4s,\testimator xgboost's best error=1.3232,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 71, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.5s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 67, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.5s,\testimator xgboost's best error=1.3232,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 72, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.6s,\testimator extra_tree's best error=1.3701,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 90, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.6s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 70, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.3s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 94, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.7s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 91, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.4s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 95, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.7s,\testimator extra_tree's best error=1.3638,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 73, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.7s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 71, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.8s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 68, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.8s,\testimator xgboost's best error=1.3232,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 74, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.9s,\testimator extra_tree's best error=1.3701,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 92, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 19.0s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 72, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 18.6s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 96, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 19.1s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 93, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 19.1s,\testimator extra_tree's best error=1.3638,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 75, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 19.2s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 94, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 19.2s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 73, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2392} INFO -  at 19.0s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:29] {2219} INFO - iteration 97, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.4s,\testimator rf's best error=1.3200,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 76, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.4s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 74, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.4s,\testimator lgbm's best error=1.3434,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 69, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.2s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 98, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.6s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 70, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.6s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 75, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.6s,\testimator lgbm's best error=1.3181,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 95, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.7s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 71, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.8s,\testimator extra_tree's best error=1.3701,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 96, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.9s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 72, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.6s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 99, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 20.0s,\testimator rf's best error=1.3200,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 77, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 19.7s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 100, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 20.1s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 97, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 20.1s,\testimator extra_tree's best error=1.3704,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 73, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 20.1s,\testimator xgboost's best error=1.3232,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 78, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 20.1s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 76, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 20.2s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 98, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2392} INFO -  at 20.3s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:30] {2219} INFO - iteration 74, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.1s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 101, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.4s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 99, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.4s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 77, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.5s,\testimator rf's best error=1.3200,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 79, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.5s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 75, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.6s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 76, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.6s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 78, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.3s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 102, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.8s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 77, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.9s,\testimator rf's best error=1.3606,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 79, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.9s,\testimator lgbm's best error=1.3181,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 100, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.9s,\testimator rf's best error=1.3200,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 80, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.6s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 103, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 21.0s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 78, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 21.0s,\testimator extra_tree's best error=1.3638,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 81, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 21.1s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 101, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 21.1s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 79, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 20.9s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 104, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 21.2s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 80, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 21.2s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 102, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 21.3s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 80, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2392} INFO -  at 21.0s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:31] {2219} INFO - iteration 105, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 21.4s,\testimator rf's best error=1.3200,\tbest estimator rf's best error=1.3200\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 82, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 21.5s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 81, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 21.5s,\testimator extra_tree's best error=1.3450,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 81, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 21.3s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 106, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 21.6s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 83, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 21.7s,\testimator extra_tree's best error=1.3450,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 82, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 21.9s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 84, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 21.9s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 82, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 21.6s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 107, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 22.0s,\testimator lgbm's best error=1.3181,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 103, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 22.0s,\testimator extra_tree's best error=1.3450,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 83, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 21.7s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 108, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 22.1s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 104, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 22.2s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 83, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 22.2s,\testimator extra_tree's best error=1.3638,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 85, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 22.2s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 84, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 22.0s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 109, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 22.3s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 105, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 22.3s,\testimator extra_tree's best error=1.3293,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 84, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2392} INFO -  at 22.4s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:32] {2219} INFO - iteration 86, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.5s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 85, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.5s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 106, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.5s,\testimator extra_tree's best error=1.3255,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 85, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.6s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 87, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.3s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 110, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.6s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 107, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.7s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 86, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.7s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 88, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.8s,\testimator extra_tree's best error=1.3255,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 86, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.8s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 108, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.6s,\testimator lgbm's best error=1.3285,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 111, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.9s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 109, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 23.0s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 87, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 23.0s,\testimator extra_tree's best error=1.3255,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 87, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 23.1s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 110, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 23.1s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 88, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 22.8s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 112, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 23.2s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 89, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2392} INFO -  at 23.3s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:33] {2219} INFO - iteration 89, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.4s,\testimator rf's best error=1.3444,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 88, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.4s,\testimator extra_tree's best error=1.3525,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 90, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.2s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 113, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.5s,\testimator lgbm's best error=1.3181,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 111, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.5s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 90, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.5s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 91, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.7s,\testimator rf's best error=1.3444,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 89, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.7s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 91, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.4s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 114, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.8s,\testimator extra_tree's best error=1.3525,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 92, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.8s,\testimator extra_tree's best error=1.3255,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 90, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.8s,\testimator extra_tree's best error=1.3701,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 112, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.5s,\testimator lgbm's best error=1.3269,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 115, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.9s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 92, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.9s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 113, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 23.7s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 116, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 24.0s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 93, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 24.2s,\testimator extra_tree's best error=1.3506,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 114, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 24.2s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 93, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 24.3s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 94, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2392} INFO -  at 24.0s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:34] {2219} INFO - iteration 117, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 24.5s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 94, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 24.5s,\testimator rf's best error=1.3284,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 115, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 24.5s,\testimator rf's best error=1.3444,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 91, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 24.7s,\testimator extra_tree's best error=1.3255,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 92, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 24.4s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 118, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 24.8s,\testimator rf's best error=1.3284,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 116, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 25.0s,\testimator rf's best error=1.3200,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 95, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 25.1s,\testimator extra_tree's best error=1.3255,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 93, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 24.8s,\testimator rf's best error=1.3517,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 119, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 25.1s,\testimator extra_tree's best error=1.3506,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 117, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 25.1s,\testimator extra_tree's best error=1.3525,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 96, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 25.2s,\testimator rf's best error=1.3363,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 95, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 25.3s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 118, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 25.3s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 97, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2392} INFO -  at 25.4s,\testimator extra_tree's best error=1.3255,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:35] {2219} INFO - iteration 94, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 25.2s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 120, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 25.6s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 96, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 25.7s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 98, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 25.8s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 97, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 25.5s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 121, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 25.9s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 99, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 25.9s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 98, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 25.7s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 122, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 26.1s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 99, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 26.2s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 95, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 26.2s,\testimator lgbm's best error=1.3181,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 119, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 26.3s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 100, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 26.3s,\testimator extra_tree's best error=1.3255,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 96, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2392} INFO -  at 26.3s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:36] {2219} INFO - iteration 120, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.1s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 123, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.5s,\testimator xgboost's best error=1.3189,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 100, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.6s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 101, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.6s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 121, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.3s,\testimator rf's best error=1.3500,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 124, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.7s,\testimator rf's best error=1.3444,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 97, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.7s,\testimator extra_tree's best error=1.3400,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 101, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.7s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 102, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.4s,\testimator lgbm's best error=1.3269,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 125, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.8s,\testimator extra_tree's best error=1.3506,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 122, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.9s,\testimator extra_tree's best error=1.3255,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 98, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.9s,\testimator xgboost's best error=1.3116,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 123, current learner rf\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 27.0s,\testimator extra_tree's best error=1.3362,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 102, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 27.2s,\testimator rf's best error=1.3170,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 124, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 26.9s,\testimator lgbm's best error=1.3269,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 126, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2392} INFO -  at 27.3s,\testimator rf's best error=1.3444,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:37] {2219} INFO - iteration 99, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 27.1s,\testimator lgbm's best error=1.3269,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 127, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 27.5s,\testimator lgbm's best error=1.3181,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 125, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 27.2s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 128, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 27.7s,\testimator lgbm's best error=1.3434,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 103, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 27.8s,\testimator lgbm's best error=1.3181,\tbest estimator xgboost's best error=1.3116\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 126, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 27.8s,\testimator lgbm's best error=1.3422,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 103, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 27.8s,\testimator extra_tree's best error=1.3237,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 129, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 28.1s,\testimator lgbm's best error=1.3244,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 100, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 28.1s,\testimator extra_tree's best error=1.3331,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 104, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 28.2s,\testimator extra_tree's best error=1.3281,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 104, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 28.3s,\testimator extra_tree's best error=1.3255,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 101, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2392} INFO -  at 28.4s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:38] {2219} INFO - iteration 105, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:39] {2392} INFO -  at 28.1s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:39] {2219} INFO - iteration 130, current learner xgboost\n",
            "[flaml.automl.logger: 06-30 11:17:39] {2392} INFO -  at 28.5s,\testimator extra_tree's best error=1.3281,\tbest estimator xgboost's best error=1.3189\n",
            "[flaml.automl.logger: 06-30 11:17:39] {2219} INFO - iteration 105, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:39] {2392} INFO -  at 28.6s,\testimator xgboost's best error=1.3272,\tbest estimator xgboost's best error=1.3272\n",
            "[flaml.automl.logger: 06-30 11:17:39] {2219} INFO - iteration 106, current learner extra_tree\n",
            "[flaml.automl.logger: 06-30 11:17:39] {2392} INFO -  at 28.6s,\testimator extra_tree's best error=1.3255,\tbest estimator lgbm's best error=1.3244\n",
            "[flaml.automl.logger: 06-30 11:17:39] {2219} INFO - iteration 102, current learner lgbm\n",
            "[flaml.automl.logger: 06-30 11:17:39] {2392} INFO -  at 28.3s,\testimator xgboost's best error=1.3221,\tbest estimator xgboost's best error=1.3221\n",
            "[flaml.automl.logger: 06-30 11:17:39] {2219} INFO - iteration 131, current learner extra_tree\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m flamly \u001b[38;5;241m=\u001b[39m make_pipeline(transformer, AutoML(time_budget\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m, early_stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m                                     eval_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv\u001b[39m\u001b[38;5;124m'\u001b[39m, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      4\u001b[0m flamld \u001b[38;5;241m=\u001b[39m make_pipeline(transformer, AutoML(time_budget\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m, early_stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                                            eval_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv\u001b[39m\u001b[38;5;124m'\u001b[39m, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflamly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflamld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfolds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[18], line 30\u001b[0m, in \u001b[0;36mdml\u001b[0;34m(X, D, y, modely, modeld, nfolds, classifier)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# out-of-fold predictions for D\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# use predict or predict_proba dependent on classifier or regressor for D\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classifier:\n\u001b[0;32m---> 30\u001b[0m     Dhat \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodeld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict_proba\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     Dhat \u001b[38;5;241m=\u001b[39m cross_val_predict(modeld, X, D, cv\u001b[38;5;241m=\u001b[39mcv, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1293\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m-> 1293\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1307\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/causalML/lib/python3.9/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from flaml import AutoML\n",
        "flamly = make_pipeline(transformer, AutoML(time_budget=100, task='regression', early_stop=True,\n",
        "                                    eval_method='cv', n_splits=3, metric='r2', verbose=3))\n",
        "flamld = make_pipeline(transformer, AutoML(time_budget=100, task='classification', early_stop=True,\n",
        "                                           eval_method='cv', n_splits=3, metric='r2', verbose=3))\n",
        "result = dml(X, D, y, flamly, flamld, nfolds=5, classifier=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2ba772",
      "metadata": {
        "id": "6e2ba772"
      },
      "outputs": [],
      "source": [
        "table = pd.concat([table , summary(*result,  X, D, y, name='automl')])\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc171ce",
      "metadata": {
        "id": "9bc171ce"
      },
      "source": [
        "# Semi-Cross-Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe9d4e9e",
      "metadata": {
        "id": "fe9d4e9e"
      },
      "source": [
        "To avoid the computational cost of performing model selection within each fold (assuming that we don't select among an exponential set of hyperparameters/models in the number of samples), it is ok to perform model selection using all the data and then perform cross-fitting with the selected model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "b6f64eba",
      "metadata": {
        "id": "b6f64eba"
      },
      "outputs": [],
      "source": [
        "flamly = make_pipeline(transformer, AutoML(time_budget=100, task='regression', early_stop=True,\n",
        "                                    eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
        "flamld = make_pipeline(transformer, AutoML(time_budget=100, task='classification', early_stop=True,\n",
        "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "2cffa502",
      "metadata": {
        "id": "2cffa502"
      },
      "outputs": [],
      "source": [
        "flamly.fit(X, y)\n",
        "besty = make_pipeline(transformer, clone(flamly[-1].best_model_for_estimator(flamly[-1].best_estimator)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "21c2a08a",
      "metadata": {
        "id": "21c2a08a"
      },
      "outputs": [],
      "source": [
        "flamld.fit(X, D)\n",
        "bestd = make_pipeline(transformer, clone(flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "6b5a1b0b",
      "metadata": {
        "id": "6b5a1b0b"
      },
      "outputs": [],
      "source": [
        "result = dml(X, D, y, besty, bestd, nfolds=5, classifier=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "9855e83e",
      "metadata": {
        "id": "9855e83e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimate</th>\n",
              "      <th>stderr</th>\n",
              "      <th>lower</th>\n",
              "      <th>upper</th>\n",
              "      <th>rmse y</th>\n",
              "      <th>rmse D</th>\n",
              "      <th>accuracy D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>double lasso</th>\n",
              "      <td>9035.120004</td>\n",
              "      <td>1295.135748</td>\n",
              "      <td>6496.653938</td>\n",
              "      <td>11573.586070</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.443406</td>\n",
              "      <td>0.688553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso/logistic</th>\n",
              "      <td>9092.461173</td>\n",
              "      <td>1304.397656</td>\n",
              "      <td>6535.841766</td>\n",
              "      <td>11649.080579</td>\n",
              "      <td>54254.468883</td>\n",
              "      <td>0.444043</td>\n",
              "      <td>0.687847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>random forest</th>\n",
              "      <td>8823.389678</td>\n",
              "      <td>1360.309746</td>\n",
              "      <td>6157.182577</td>\n",
              "      <td>11489.596779</td>\n",
              "      <td>55152.962053</td>\n",
              "      <td>0.444324</td>\n",
              "      <td>0.688553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decision tree</th>\n",
              "      <td>9237.030821</td>\n",
              "      <td>1440.550571</td>\n",
              "      <td>6413.551700</td>\n",
              "      <td>12060.509941</td>\n",
              "      <td>59427.226868</td>\n",
              "      <td>0.446437</td>\n",
              "      <td>0.688048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>boosted forest</th>\n",
              "      <td>9082.510152</td>\n",
              "      <td>1336.345327</td>\n",
              "      <td>6463.273310</td>\n",
              "      <td>11701.746994</td>\n",
              "      <td>55593.735380</td>\n",
              "      <td>0.443375</td>\n",
              "      <td>0.690066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>automl (semi-cfit)</th>\n",
              "      <td>8884.424774</td>\n",
              "      <td>1306.176253</td>\n",
              "      <td>6324.319318</td>\n",
              "      <td>11444.530230</td>\n",
              "      <td>53908.841872</td>\n",
              "      <td>0.443595</td>\n",
              "      <td>0.690973</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       estimate       stderr        lower         upper  \\\n",
              "double lasso        9035.120004  1295.135748  6496.653938  11573.586070   \n",
              "lasso/logistic      9092.461173  1304.397656  6535.841766  11649.080579   \n",
              "random forest       8823.389678  1360.309746  6157.182577  11489.596779   \n",
              "decision tree       9237.030821  1440.550571  6413.551700  12060.509941   \n",
              "boosted forest      9082.510152  1336.345327  6463.273310  11701.746994   \n",
              "automl (semi-cfit)  8884.424774  1306.176253  6324.319318  11444.530230   \n",
              "\n",
              "                          rmse y    rmse D  accuracy D  \n",
              "double lasso        54254.468883  0.443406    0.688553  \n",
              "lasso/logistic      54254.468883  0.444043    0.687847  \n",
              "random forest       55152.962053  0.444324    0.688553  \n",
              "decision tree       59427.226868  0.446437    0.688048  \n",
              "boosted forest      55593.735380  0.443375    0.690066  \n",
              "automl (semi-cfit)  53908.841872  0.443595    0.690973  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table = pd.concat([table , summary(*result,  X, D, y, name='automl (semi-cfit)')])\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b97cb96c",
      "metadata": {
        "id": "b97cb96c"
      },
      "source": [
        "### Semi-Crossfitting with Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5214c9d",
      "metadata": {
        "id": "f5214c9d"
      },
      "outputs": [],
      "source": [
        "def dml_dirty(X, D, y, modely_list, modeld_list, *,\n",
        "              stacker=LinearRegression(), nfolds, classifier=False):\n",
        "    '''\n",
        "    DML for the Partially Linear Model setting with semi-cross-fitting\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "    X: the controls\n",
        "    D: the treatment\n",
        "    y: the outcome\n",
        "    modely: the ML model for predicting the outcome y\n",
        "    modeld: the ML model for predicting the treatment D\n",
        "    stacker: model used to aggregate predictions of each of the base models\n",
        "    nfolds: the number of folds in cross-fitting\n",
        "    classifier: bool, whether the modeld is a classifier or a regressor\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "    point: the point estimate of the treatment effect of D on y\n",
        "    stderr: the standard error of the treatment effect\n",
        "    yhat: the cross-fitted predictions for the outcome y\n",
        "    Dhat: the cross-fitted predictions for the treatment D\n",
        "    resy: the outcome residuals\n",
        "    resD: the treatment residuals\n",
        "    epsilon: the final residual-on-residual OLS regression residual\n",
        "    '''\n",
        "    # construct out-of-fold predictions for each model\n",
        "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
        "    yhats = np.array([cross_val_predict(modely, X, y, cv=cv, n_jobs=-1) for modely in modely_list]).T\n",
        "    if classifier:\n",
        "        Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
        "                         for modeld in modeld_list]).T\n",
        "    else:\n",
        "        Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1) for modeld in modeld_list]).T\n",
        "    # calculate stacked residuals by finding optimal coefficients\n",
        "    # and weigthing out-of-sample predictions by these coefficients\n",
        "    yhat = stacker.fit(yhats, y).predict(yhats)\n",
        "    Dhat = stacker.fit(Dhats, D).predict(Dhats)\n",
        "    resy = y - yhat\n",
        "    resD = D - Dhat\n",
        "    # go with the stacked residuals\n",
        "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
        "    epsilon = resy - point * resD\n",
        "    var = np.mean(epsilon**2 * resD**2) / np.mean(resD**2)**2\n",
        "    stderr = np.sqrt(var / X.shape[0])\n",
        "    return point, stderr, yhat, Dhat, resy, resD, epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a671f5e",
      "metadata": {
        "id": "6a671f5e"
      },
      "outputs": [],
      "source": [
        "result = dml_dirty(X, D, y, [lassoy, rfy, dtry, gbfy], [lgrd, rfd, dtrd, gbfd],\n",
        "                   nfolds=5, classifier=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6bfa361",
      "metadata": {
        "id": "f6bfa361"
      },
      "outputs": [],
      "source": [
        "table = pd.concat([table , summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comprehensive-graphics",
      "metadata": {
        "id": "comprehensive-graphics"
      },
      "source": [
        "## Interactive Regression Model (IRM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "square-craps",
      "metadata": {
        "id": "square-craps"
      },
      "source": [
        "Next, we consider estimation of average treatment effects when treatment effects are fully heterogeneous:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "large-welcome",
      "metadata": {
        "id": "large-welcome"
      },
      "source": [
        " \\begin{eqnarray}\\label{eq: HetPL1}\n",
        " & Y  = g_0(D, X) + U,  &  \\quad E[U \\mid X, D]= 0,\\\\\n",
        "  & D  = m_0(X) + V,  & \\quad  E[V\\mid X] = 0.\n",
        "\\end{eqnarray}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "perfect-reliance",
      "metadata": {
        "id": "perfect-reliance"
      },
      "source": [
        "To reduce the disproportionate impact of extreme propensity score weights in the interactive model\n",
        "we trim the propensity scores which are below .01 or above .99."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a0b5d47",
      "metadata": {
        "id": "5a0b5d47"
      },
      "outputs": [],
      "source": [
        "def dr(X, D, y, modely0, modely1, modeld, *, trimming=0.01, nfolds):\n",
        "    '''\n",
        "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
        "    with cross-fitting\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "    X: the controls\n",
        "    D: the treatment\n",
        "    y: the outcome\n",
        "    modely0: the ML model for predicting the outcome y in the control population\n",
        "    modely1: the ML model for predicting the outcome y in the treated population\n",
        "    modeld: the ML model for predicting the treatment D\n",
        "    trimming: threshold below which to trim propensities\n",
        "    nfolds: the number of folds in cross-fitting\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "    point: the point estimate of the treatment effect of D on y\n",
        "    stderr: the standard error of the treatment effect\n",
        "    yhat: the cross-fitted predictions for the outcome y\n",
        "    Dhat: the cross-fitted predictions for the outcome D\n",
        "    resy: the outcome residuals\n",
        "    resD: the treatment residuals\n",
        "    drhat: the doubly robust quantity for each sample\n",
        "    '''\n",
        "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
        "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
        "    # we will fit a model E[Y| D, X] by fitting a separate model for D==0\n",
        "    # and a separate model for D==1.\n",
        "    for train, test in cv.split(X, y):\n",
        "        # train a model on training data that received treatment zero and predict on all data in test set\n",
        "        yhat0[test] = clone(modely0).fit(X.iloc[train][D[train]==0], y[train][D[train]==0]).predict(X.iloc[test])\n",
        "        # train a model on training data that received treatment one and predict on all data in test set\n",
        "        yhat1[test] = clone(modely1).fit(X.iloc[train][D[train]==1], y[train][D[train]==1]).predict(X.iloc[test])\n",
        "    # prediction for observed treatment\n",
        "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
        "    # propensity scores\n",
        "    Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
        "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
        "    # doubly robust quantity for every sample\n",
        "    drhat = yhat1 - yhat0 + (y - yhat) * (D/Dhat - (1 - D)/(1 - Dhat))\n",
        "    point = np.mean(drhat)\n",
        "    var = np.var(drhat)\n",
        "    stderr = np.sqrt(var / X.shape[0])\n",
        "    return point, stderr, yhat, Dhat, y - yhat, D - Dhat, drhat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MRgN7S5TXlfF",
      "metadata": {
        "id": "MRgN7S5TXlfF"
      },
      "source": [
        "**NB**: There is randomness across the random seed that potentially causes instability in our estimates. In particular, we find that the lasso/logistic specification for the IRM model exhibits great instability across different seeds. To mitigate these differences, we can take an average across seeds. In principle we would do this for all estimates, but for computation and simplicity, we cheat a little bit and do it only for the model specification we know varies significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZpHWxbk7L7Sl",
      "metadata": {
        "id": "ZpHWxbk7L7Sl"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "lassoytest = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
        "lgrdtest = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
        "result = dr(X, D, y, lassoytest, lassoytest, lgrdtest, nfolds=5)\n",
        "seed_estimates = summary(*result,  X, D, y, name='lasso/logistic')\n",
        "\n",
        "for i in range(9):\n",
        "  cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
        "  lassoytest = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
        "  lgrdtest = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
        "  result = dr(X, D, y, lassoytest, lassoytest, lgrdtest, nfolds=5)\n",
        "  seed_estimates = pd.concat([seed_estimates , summary(*result,  X, D, y, name='lasso/logistic')])\n",
        "\n",
        "seed_estimates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2szrjAObipxP",
      "metadata": {
        "id": "2szrjAObipxP"
      },
      "source": [
        "As we can see, using lasso and logistic regression for $Y$ and $D$ respectively leads to estimates that vary across different seeds. Letting $\\hat{\\theta}_s$ denote the vector of $s=10$ estimates across the seeds, we take $$\\hat{\\theta} := \\text{median}(\\hat{\\theta}_s)$$ and $$SE(\\hat{\\theta}) := \\sqrt{\\text{median}\\left(SE(\\hat{\\theta}_s)^2 + (\\hat{\\theta}_s - \\hat{\\theta})^2\\right)}$$ (Note in the standard error calculation, the operation of de-medianing $\\hat{\\theta}_s - \\hat{\\theta}$ is broadcasted.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zQnSP-hDlVPw",
      "metadata": {
        "id": "zQnSP-hDlVPw"
      },
      "outputs": [],
      "source": [
        "med_theta = np.median(seed_estimates.values[:,0])\n",
        "se_med = np.sqrt(np.median((seed_estimates.values[:,1])**2 + (seed_estimates.values[:,0] - med_theta)**2))\n",
        "tabledr = pd.DataFrame({'estimate': med_theta,\n",
        "                        'stderr': se_med,\n",
        "                        'lower': med_theta - 1.96*se_med,\n",
        "                        'upper': med_theta + 1.96*se_med,\n",
        "                        'rmse y': np.median(seed_estimates.values[:,4]),\n",
        "                        'rmse D': np.median(seed_estimates.values[:,5]),\n",
        "                        'accuracy D': np.median(seed_estimates.values[:,6]),\n",
        "                        }, index=['lasso/logistic'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0fe147",
      "metadata": {
        "id": "1b0fe147"
      },
      "outputs": [],
      "source": [
        "rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
        "rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
        "result = dr(X, D, y, rfy, rfy, rfd, nfolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259efb1c",
      "metadata": {
        "id": "259efb1c"
      },
      "outputs": [],
      "source": [
        "tabledr = pd.concat([tabledr , summary(*result,  X, D, y, name='random forest')])\n",
        "tabledr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5335633c",
      "metadata": {
        "id": "5335633c"
      },
      "outputs": [],
      "source": [
        "dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
        "dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
        "result = dr(X, D, y, dtry, dtry, dtrd, nfolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c5d5a12",
      "metadata": {
        "id": "3c5d5a12"
      },
      "outputs": [],
      "source": [
        "tabledr = pd.concat([tabledr , summary(*result,  X, D, y, name='decision tree')])\n",
        "tabledr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f9338c",
      "metadata": {
        "id": "69f9338c"
      },
      "outputs": [],
      "source": [
        "gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
        "gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
        "result = dr(X, D, y, gbfy, gbfy, gbfd, nfolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d275ed3",
      "metadata": {
        "id": "8d275ed3"
      },
      "outputs": [],
      "source": [
        "tabledr = pd.concat([tabledr , summary(*result, X, D, y, name='boosted forest')])\n",
        "tabledr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a6e0994",
      "metadata": {
        "id": "9a6e0994"
      },
      "source": [
        "These estimates that flexibly account for confounding are substantially attenuated relative to the baseline estimate (19559) that does not account for confounding. They suggest much smaller causal effects of 401(k) eligiblity on financial asset holdings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8eadf06",
      "metadata": {
        "id": "a8eadf06"
      },
      "source": [
        "# Semi-Cross-Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fee26e18",
      "metadata": {
        "id": "fee26e18"
      },
      "outputs": [],
      "source": [
        "from flaml import AutoML\n",
        "flamly0 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
        "                                     eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
        "flamly1 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
        "                                     eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
        "flamld = make_pipeline(transformer, AutoML(time_budget=60, task='classification', early_stop=True,\n",
        "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22518880",
      "metadata": {
        "id": "22518880"
      },
      "outputs": [],
      "source": [
        "flamly0.fit(X[D==0], y[D==0])\n",
        "besty0 = make_pipeline(transformer, clone(flamly0[-1].best_model_for_estimator(flamly0[-1].best_estimator)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd04a3cd",
      "metadata": {
        "id": "dd04a3cd"
      },
      "outputs": [],
      "source": [
        "flamly1.fit(X[D==1], y[D==1])\n",
        "besty1 = make_pipeline(transformer, clone(flamly1[-1].best_model_for_estimator(flamly1[-1].best_estimator)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5088756e",
      "metadata": {
        "id": "5088756e"
      },
      "outputs": [],
      "source": [
        "flamld.fit(X, D)\n",
        "bestd = make_pipeline(transformer, clone(flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a415bd9",
      "metadata": {
        "id": "9a415bd9"
      },
      "outputs": [],
      "source": [
        "result = dr(X, D, y, besty0, besty1, bestd, nfolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30800bbc",
      "metadata": {
        "id": "30800bbc"
      },
      "outputs": [],
      "source": [
        "tabledr = pd.concat([tabledr , summary(*result,  X, D, y, name='automl (semi-cfit)')])\n",
        "tabledr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9325941a",
      "metadata": {
        "id": "9325941a"
      },
      "outputs": [],
      "source": [
        "def dr_dirty(X, D, y, modely0_list, modely1_list, modeld_list, *,\n",
        "             stacker=LinearRegression(), trimming=0.01, nfolds):\n",
        "    '''\n",
        "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
        "    with cross-fitting\n",
        "\n",
        "    Input\n",
        "    -----\n",
        "    X: the controls\n",
        "    D: the treatment\n",
        "    y: the outcome\n",
        "    modely_list: list of ML models for predicting the outcome y\n",
        "    modeld_list: list of ML models for predicting the treatment D\n",
        "    stacker: model used to aggregate predictions of each of the base models\n",
        "    trimming: threshold below which to trim propensities\n",
        "    nfolds: the number of folds in cross-fitting\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "    point: the point estimate of the treatment effect of D on y\n",
        "    stderr: the standard error of the treatment effect\n",
        "    yhat: the cross-fitted predictions for the outcome y\n",
        "    Dhat: the cross-fitted predictions for the outcome D\n",
        "    resy: the outcome residuals\n",
        "    resD: the treatment residuals\n",
        "    drhat: the doubly robust quantity for each sample\n",
        "    '''\n",
        "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
        "\n",
        "    # we will fit a model E[Y| D, X] by fitting a separate model for D==0\n",
        "    # and a separate model for D==1. We do that for each model type in modely_list\n",
        "    yhats0, yhats1 = np.zeros((y.shape[0], len(modely0_list))), np.zeros((y.shape[0], len(modely1_list)))\n",
        "    for train, test in cv.split(X, y):\n",
        "        for it, modely0 in enumerate(modely0_list):\n",
        "            yhats0[test, it] = clone(modely0).fit(X.iloc[train][D[train]==0], y[train][D[train]==0]).predict(X.iloc[test])\n",
        "        for it, modely1 in enumerate(modely1_list):\n",
        "            yhats1[test, it] = clone(modely1).fit(X.iloc[train][D[train]==1], y[train][D[train]==1]).predict(X.iloc[test])\n",
        "\n",
        "    # calculate stacking weights for the outcome model for each population\n",
        "    # and combine the outcome model predictions\n",
        "    yhat0 = clone(stacker).fit(yhats0[D==0], y[D==0]).predict(yhats0)\n",
        "    yhat1 = clone(stacker).fit(yhats1[D==1], y[D==1]).predict(yhats1)\n",
        "\n",
        "    # prediction for observed treatment using the stacked model\n",
        "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
        "\n",
        "    # propensity scores\n",
        "    Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
        "                     for modeld in modeld_list]).T\n",
        "    # construct coefficients on each model based on stacker\n",
        "    Dhat = clone(stacker).fit(Dhats, D).predict(Dhats)\n",
        "    # trim propensities\n",
        "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
        "\n",
        "    # doubly robust quantity for every sample\n",
        "    drhat = yhat1 - yhat0 + (y - yhat) * (D/Dhat - (1 - D)/(1 - Dhat))\n",
        "    point = np.mean(drhat)\n",
        "    var = np.var(drhat)\n",
        "    stderr = np.sqrt(var / X.shape[0])\n",
        "    return point, stderr, yhat, Dhat, y - yhat, D - Dhat, drhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5401f09d",
      "metadata": {
        "id": "5401f09d"
      },
      "outputs": [],
      "source": [
        "result = dr_dirty(X, D, y, [lassoy, rfy, dtry, gbfy], [lassoy, rfy, dtry, gbfy], [lgrd, rfd, dtrd, gbfd], nfolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1606f1bb",
      "metadata": {
        "id": "1606f1bb"
      },
      "outputs": [],
      "source": [
        "tabledr = pd.concat([tabledr , summary(*result,  X, D, y, name='stacked (semi-cfit)')])\n",
        "tabledr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "743cde38",
      "metadata": {
        "id": "743cde38"
      },
      "source": [
        "We can compare the results between the PLR model and IRM model. We find that the effect under the IRM model is typically of lower value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce30440",
      "metadata": {
        "id": "0ce30440"
      },
      "outputs": [],
      "source": [
        "table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93fcee1c",
      "metadata": {
        "id": "93fcee1c"
      },
      "outputs": [],
      "source": [
        "tabledr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af5867dc",
      "metadata": {
        "id": "af5867dc"
      },
      "source": [
        "# Using the EconML Library\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MCC8KTmCoLOZ",
      "metadata": {
        "id": "MCC8KTmCoLOZ"
      },
      "source": [
        "We are interested in valid estimators of the average treatment effect of `e401` on `net_tfa`. There exist nice packages out there that can help us do our estimation with the simple call of a function. Such packages include `EconML` (Python), `DoubleML` (Python and R), and `ddml` (Stata and R).\n",
        "\n",
        "We run through PLR and IRM using `EconML` below to illustrate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "401bc6b6",
      "metadata": {
        "id": "401bc6b6"
      },
      "outputs": [],
      "source": [
        "# code verified with this version of econml\n",
        "# If you want to try out the latest econml version remove the version number;\n",
        "# albeit mistakes could potentially be raised due to library updates”\n",
        "!pip install econml==0.14.1 #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd88f6b",
      "metadata": {
        "id": "ebd88f6b"
      },
      "outputs": [],
      "source": [
        "# for these libraries we will just pre-featurize the controls\n",
        "W = StandardScaler().fit_transform(transformer.fit_transform(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe3ba9f4",
      "metadata": {
        "id": "fe3ba9f4"
      },
      "outputs": [],
      "source": [
        "from econml.dml import LinearDML\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "ldml = LinearDML(model_y=LassoCV(cv=cv), model_t=LogisticRegressionCV(cv=cv),\n",
        "                 cv=3, discrete_treatment=True, random_state=123).fit(y, D, W=W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a52d86c5",
      "metadata": {
        "id": "a52d86c5"
      },
      "outputs": [],
      "source": [
        "ldml.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6c5b24",
      "metadata": {
        "id": "4d6c5b24"
      },
      "outputs": [],
      "source": [
        "# r2scores\n",
        "r2scorey = np.mean(ldml.nuisance_scores_y)\n",
        "r2scored = np.mean(ldml.nuisance_scores_t)\n",
        "r2scorey, r2scored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "473f350b",
      "metadata": {
        "id": "473f350b"
      },
      "outputs": [],
      "source": [
        "from econml.dr import LinearDRLearner\n",
        "\n",
        "# dr learner in econml fits a single regression function of Y from X, D\n",
        "# using all the data\n",
        "dr = LinearDRLearner(model_regression=LassoCV(cv=cv),\n",
        "                     model_propensity=LogisticRegressionCV(cv=cv),\n",
        "                     cv=3, min_propensity=0.01, random_state=123).fit(y, D, W=W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea53e99a",
      "metadata": {
        "id": "ea53e99a"
      },
      "outputs": [],
      "source": [
        "dr.summary(T=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "519426af",
      "metadata": {
        "id": "519426af"
      },
      "outputs": [],
      "source": [
        "from econml.dr import LinearDRLearner\n",
        "from econml.utilities import SeparateModel\n",
        "\n",
        "# to implement the separate regression models we need use the separate model wrapper\n",
        "# that splits the data based on the last covariate (in this case D) and fits a separate\n",
        "# model for each group. The input to the wrapper is the model to use for each value\n",
        "# of the last covariate.\n",
        "dr = LinearDRLearner(model_regression=SeparateModel(LassoCV(cv=cv), LassoCV(cv=cv)),\n",
        "                     model_propensity=LogisticRegressionCV(cv=cv),\n",
        "                     cv=3, min_propensity=0.01, random_state=123).fit(y, D, W=W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3794606",
      "metadata": {
        "id": "e3794606"
      },
      "outputs": [],
      "source": [
        "dr.summary(T=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca94a11",
      "metadata": {
        "id": "3ca94a11"
      },
      "source": [
        "# Using the DoubleML library"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qj9BrMhWpk70",
      "metadata": {
        "id": "qj9BrMhWpk70"
      },
      "source": [
        "We now play with `DoubleML` to illustrate its use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ea90e4e1",
      "metadata": {
        "id": "ea90e4e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting doubleml\n",
            "  Downloading DoubleML-0.8.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: joblib in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from doubleml) (1.3.2)\n",
            "Requirement already satisfied: numpy in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from doubleml) (1.26.4)\n",
            "Requirement already satisfied: pandas in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from doubleml) (2.2.2)\n",
            "Requirement already satisfied: scipy in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from doubleml) (1.12.0)\n",
            "Requirement already satisfied: scikit-learn in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from doubleml) (1.4.1.post1)\n",
            "Requirement already satisfied: statsmodels in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from doubleml) (0.14.1)\n",
            "Collecting plotly (from doubleml)\n",
            "  Downloading plotly-5.23.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from pandas->doubleml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from pandas->doubleml) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from pandas->doubleml) (2024.1)\n",
            "Collecting tenacity>=6.2.0 (from plotly->doubleml)\n",
            "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: packaging in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from plotly->doubleml) (23.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from scikit-learn->doubleml) (3.3.0)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from statsmodels->doubleml) (0.5.6)\n",
            "Requirement already satisfied: six in /Users/pablo/opt/anaconda3/envs/causalML/lib/python3.9/site-packages (from patsy>=0.5.4->statsmodels->doubleml) (1.16.0)\n",
            "Downloading DoubleML-0.8.1-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.5/274.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plotly-5.23.0-py3-none-any.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: tenacity, plotly, doubleml\n",
            "Successfully installed doubleml-0.8.1 plotly-5.23.0 tenacity-9.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install doubleml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9930faf2",
      "metadata": {
        "id": "9930faf2"
      },
      "outputs": [],
      "source": [
        "from doubleml import DoubleMLData\n",
        "dml_data = DoubleMLData.from_arrays(W, y, D)\n",
        "print(dml_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c146dd15",
      "metadata": {
        "id": "c146dd15"
      },
      "outputs": [],
      "source": [
        "import doubleml as dml\n",
        "\n",
        "dml_plr_obj = dml.DoubleMLPLR(dml_data, LassoCV(cv=cv), LogisticRegressionCV(cv=cv), n_folds=3)\n",
        "print(dml_plr_obj.fit())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e7b7b7c",
      "metadata": {
        "id": "9e7b7b7c"
      },
      "outputs": [],
      "source": [
        "dml_irm_obj = dml.DoubleMLIRM(dml_data, LassoCV(cv=cv), LogisticRegressionCV(cv=cv), n_folds=3)\n",
        "\n",
        "print(dml_irm_obj.fit())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 537.547458,
      "end_time": "2021-03-24T14:22:44.931595",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-03-24T14:13:47.384137",
      "version": "2.3.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
